# =====================================================
# Scout Lakehouse - Silver ETL Transformation
# PySpark Notebook for Microsoft Fabric Lakehouse
# Bronze ‚Üí Silver with JSON explosion and dimensional modeling
# =====================================================

# Import required libraries
from pyspark.sql import functions as F, types as T
from pyspark.sql.window import Window
from datetime import datetime, timedelta
import json

print(f"Starting Silver transformation at: {datetime.now()}")
print("Processing: Bronze ‚Üí Silver with JSON explosion")

# =====================================================
# LOAD BRONZE TABLES
# =====================================================

print("üì• Loading Bronze layer tables...")

# Load Bronze sales interactions
si = spark.read.table("bronze.sales_interactions_raw") \
    .withColumn("canonical_tx_id", F.lower(F.col("canonical_tx_id")))

# Load Bronze payload transactions with JSON
pt = spark.read.table("bronze.payload_transactions_raw") \
    .withColumn("canonical_tx_id", F.lower(F.col("canonical_tx_id")))

# Load reference data
stores_raw = spark.read.table("bronze.stores_raw")
brands_raw = spark.read.table("bronze.brands_raw")
categories_raw = spark.read.table("bronze.categories_raw")

bronze_counts = {
    "sales_interactions": si.count(),
    "payload_transactions": pt.count(),
    "stores": stores_raw.count(),
    "brands": brands_raw.count(),
    "categories": categories_raw.count()
}

print(f"üìä Bronze data loaded:")
for table, count in bronze_counts.items():
    print(f"  - {table}: {count:,} rows")

# =====================================================
# JSON EXPLOSION - PAYLOAD TRANSACTIONS
# =====================================================

print("üîç Exploding JSON payload items...")

# Explode JSON items array to SKU level
items = (pt
    .filter(F.col("payload_json").isNotNull())
    .withColumn("items_arr",
        F.from_json(
            F.get_json_object("payload_json", "$.items"),
            T.ArrayType(T.MapType(T.StringType(), T.StringType()))
        )
    )
    .withColumn("item", F.explode_outer("items_arr"))
    .select(
        F.col("canonical_tx_id"),
        F.col("storeId").cast("int").alias("store_id"),
        F.col("amount").cast("decimal(18,2)").alias("transaction_total"),
        F.get_json_object("payload_json", "$.customer.facialId").alias("facial_id_json"),
        F.get_json_object("payload_json", "$.customer.age").cast("int").alias("customer_age_json"),
        F.get_json_object("payload_json", "$.customer.gender").alias("customer_gender_json"),
        F.get_json_object("payload_json", "$.timestamp").alias("event_ts"),
        # Item details
        F.col("item")["sku"].alias("sku"),
        F.col("item")["brand"].alias("item_brand"),
        F.col("item")["category"].alias("item_category"),
        F.col("item")["quantity"].cast("int").alias("item_qty"),
        F.col("item")["unitPrice"].cast("decimal(18,2)").alias("item_unit_price"),
        F.col("item")["total"].cast("decimal(18,2)").alias("item_total"),
        # Substitution handling
        F.col("item")["isSubstitution"].cast("boolean").alias("is_substitution"),
        F.col("item")["originalSku"].alias("original_sku"),
        F.col("item")["substitutionReason"].alias("substitution_reason")
    )
    .filter(F.col("sku").isNotNull())  # Valid SKUs only
    .withColumn("item_sequence", F.row_number().over(
        Window.partitionBy("canonical_tx_id").orderBy("sku")
    ))
)

# Add Nielsen category mappings to items
items_with_nielsen = items \
    .withColumn("nielsen_l1",
        F.when(F.lower(F.col("item_category")).contains("tobacco"), "Tobacco Products")
         .when(F.lower(F.col("item_category")).contains("laundry"), "Household Care")
         .when(F.lower(F.col("item_brand")).contains("downy"), "Household Care")
         .when(F.lower(F.col("item_brand")).contains("surf"), "Household Care")
         .otherwise("Food & Beverages")
    ) \
    .withColumn("nielsen_l2",
        F.when(F.col("nielsen_l1") == "Tobacco Products", "Cigarettes")
         .when(F.col("nielsen_l1") == "Household Care", "Laundry Care")
         .otherwise("Packaged Food")
    ) \
    .withColumn("nielsen_l3",
        F.when(F.col("nielsen_l1") == "Tobacco Products", "Regular Cigarettes")
         .when(F.col("nielsen_l1") == "Household Care", "Fabric Softeners")
         .otherwise("Snacks")
    )

# Save transaction items
items_with_nielsen.write.mode("overwrite").saveAsTable("silver.transaction_items")

items_count = items_with_nielsen.count()
print(f"üì¶ Transaction items created: {items_count:,} SKU-level items")

# =====================================================
# SILVER TRANSACTIONS (SINGLE DATE AUTHORITY)
# =====================================================

print("üí∞ Creating Silver transactions with single date authority...")

# Create core transactions with single date authority
tx = (si
    .select(
        F.col("canonical_tx_id"),
        F.col("interaction_id"),
        F.col("store_id").cast("int"),
        F.col("customer_id").alias("facial_id"),
        F.col("transaction_date").cast("date").alias("transaction_date"),   # SINGLE DATE AUTHORITY
        F.col("transaction_time"),
        F.col("date_key").cast("int"),
        F.col("time_key").cast("int"),
        F.col("device_id"),
        F.col("age").cast("int").alias("customer_age"),
        F.col("gender").alias("customer_gender"),
        F.col("emotional_state"),
        F.col("transcription_text").alias("transcript_text"),
        F.col("barangay_id").cast("int"),
        F.col("persona_rule_id").cast("int"),
        F.col("assigned_persona").alias("persona_assigned"),
        F.col("created_date").cast("timestamp").alias("created_ts")
    )
    # Join with payload data for additional fields
    .join(
        pt.select("canonical_tx_id", "amount", "payload_json"),
        "canonical_tx_id",
        "left"
    )
    # Calculate transaction value (prefer SalesInteractionFact, fallback to PayloadTransactions)
    .withColumn("transaction_value",
        F.coalesce(
            F.col("transaction_value"),
            F.col("amount").cast("decimal(18,2)")
        )
    )
    # Calculate basket size from items
    .join(
        items_with_nielsen.groupBy("canonical_tx_id").agg(
            F.sum("item_qty").alias("basket_size_calculated"),
            F.count("*").alias("unique_skus")
        ),
        "canonical_tx_id",
        "left"
    )
    .withColumn("basket_size",
        F.coalesce(F.col("basket_size_calculated"), F.lit(1))
    )
    # Add time-based derived fields
    .withColumn("hour_24", F.hour("created_ts"))
    .withColumn("weekday_vs_weekend",
        F.when(F.dayofweek("transaction_date").isin(1, 7), "Weekend")
         .otherwise("Weekday")
    )
    .withColumn("time_of_day_category",
        F.when(F.col("hour_24").between(6, 8), "Early-Morning")
         .when(F.col("hour_24").between(9, 11), "Late-Morning")
         .when(F.col("hour_24").between(12, 14), "Lunch-Time")
         .when(F.col("hour_24").between(15, 17), "Afternoon")
         .when(F.col("hour_24").between(18, 20), "Evening")
         .when(F.col("hour_24").between(21, 23), "Night")
         .otherwise("Late-Night")
    )
    .withColumn("business_time_period",
        F.when(F.col("hour_24").between(7, 9), "Rush-Hour-Morning")
         .when(F.col("hour_24").between(10, 16), "Business-Hours")
         .when(F.col("hour_24").between(17, 19), "Rush-Hour-Evening")
         .when(F.col("hour_24").between(20, 22), "Prime-Time")
         .otherwise("Off-Peak")
    )
    # Add conversation scoring (mock implementation)
    .withColumn("conversation_score",
        F.when(F.col("emotional_state") == "Happy", F.rand() * 3 + 7)
         .when(F.col("emotional_state") == "Satisfied", F.rand() * 2 + 6)
         .when(F.col("emotional_state") == "Neutral", F.rand() * 4 + 4)
         .when(F.col("emotional_state") == "Frustrated", F.rand() * 3 + 2)
         .otherwise(F.rand() * 10)
    )
    # Add substitution flag
    .withColumn("was_substitution",
        F.coalesce(
            F.col("was_substitution"),
            F.lit(False)
        )
    )
    # Add persona confidence scoring
    .withColumn("persona_confidence",
        F.when(F.col("persona_assigned").isNotNull(), F.rand() * 0.3 + 0.7)
         .otherwise(F.lit(0.5))
    )
    # Clean up columns
    .drop("amount", "payload_json", "basket_size_calculated")
)

# Save Silver transactions
tx.write.mode("overwrite").saveAsTable("silver.transactions")

tx_count = tx.count()
unique_tx_count = tx.select("canonical_tx_id").distinct().count()
print(f"üí∞ Silver transactions created: {tx_count:,} rows ({unique_tx_count:,} unique)")

# =====================================================
# DIMENSION TABLES
# =====================================================

print("üìä Creating dimension tables...")

# Store dimension
dim_store = stores_raw \
    .select(
        F.col("store_id").cast("int"),
        F.col("store_name"),
        F.col("region_name"),
        F.col("province_name"),
        F.col("municipality_name"),
        F.col("barangay_name"),
        F.coalesce(F.col("store_type"), F.lit("Retail")).alias("store_type"),
        F.col("latitude").cast("double"),
        F.col("longitude").cast("double"),
        F.coalesce(F.col("is_active"), F.lit(True)).alias("is_active"),
        F.current_timestamp().alias("created_date")
    ) \
    .dropDuplicates(["store_id"])

dim_store.write.mode("overwrite").saveAsTable("silver.dim_store")

# Brand dimension with Nielsen mappings
dim_brand = brands_raw \
    .select(
        F.col("brand_id").cast("int"),
        F.col("brand_name"),
        F.col("brand_category"),
        F.col("nielsen_l1_category"),
        F.col("nielsen_l2_category"),
        F.col("nielsen_l3_category"),
        F.coalesce(F.col("is_premium"), F.lit(False)).alias("is_premium"),
        F.coalesce(F.col("market_segment"), F.lit("Mass")).alias("market_segment"),
        F.when(F.lower(F.col("nielsen_l1_category")).contains("tobacco"), True).otherwise(False).alias("is_tobacco"),
        F.when(F.lower(F.col("nielsen_l2_category")).contains("laundry"), True).otherwise(False).alias("is_laundry"),
        F.current_timestamp().alias("created_date")
    ) \
    .dropDuplicates(["brand_id"])

dim_brand.write.mode("overwrite").saveAsTable("silver.dim_brand")

# Category dimension
dim_category = categories_raw \
    .select(
        F.col("category_id").cast("int"),
        F.col("category_name"),
        F.col("parent_category_id").cast("int"),
        F.coalesce(F.col("category_level"), F.lit(1)).alias("category_level"),
        F.col("nielsen_mapping"),
        F.when(F.lower(F.col("category_name")).contains("tobacco"), True).otherwise(False).alias("is_tobacco"),
        F.when(F.lower(F.col("category_name")).contains("laundry"), True).otherwise(False).alias("is_laundry"),
        F.current_timestamp().alias("created_date")
    ) \
    .dropDuplicates(["category_id"])

dim_category.write.mode("overwrite").saveAsTable("silver.dim_category")

# Date dimension (generate date range)
date_range = tx.select(F.min("transaction_date").alias("min_date"), F.max("transaction_date").alias("max_date")).collect()[0]
start_date = date_range["min_date"]
end_date = date_range["max_date"]

if start_date and end_date:
    # Generate date dimension
    date_df = spark.range(0, (end_date - start_date).days + 1) \
        .select(F.date_add(F.lit(start_date), F.col("id").cast("int")).alias("full_date")) \
        .withColumn("date_key", F.date_format("full_date", "yyyyMMdd").cast("int")) \
        .withColumn("year", F.year("full_date")) \
        .withColumn("quarter", F.quarter("full_date")) \
        .withColumn("month", F.month("full_date")) \
        .withColumn("month_name", F.date_format("full_date", "MMMM")) \
        .withColumn("day_of_month", F.dayofmonth("full_date")) \
        .withColumn("day_of_week", F.dayofweek("full_date")) \
        .withColumn("day_name", F.date_format("full_date", "EEEE")) \
        .withColumn("is_weekend", F.col("day_of_week").isin(1, 7)) \
        .withColumn("is_holiday", F.lit(False))  # TODO: Add actual holiday logic \
        .withColumn("fiscal_year", F.col("year"))  # Assuming calendar year = fiscal year \
        .withColumn("fiscal_quarter", F.col("quarter")) \
        .withColumn("fiscal_month", F.col("month"))

    date_df.write.mode("overwrite").saveAsTable("silver.dim_date")

# Time dimension (24 hours)
time_df = spark.range(0, 24) \
    .select(F.col("id").cast("int").alias("hour_24")) \
    .withColumn("time_key", F.col("hour_24") * 100)  # Simple time key \
    .withColumn("minute", F.lit(0)) \
    .withColumn("time_of_day_category",
        F.when(F.col("hour_24").between(6, 8), "Early-Morning")
         .when(F.col("hour_24").between(9, 11), "Late-Morning")
         .when(F.col("hour_24").between(12, 14), "Lunch-Time")
         .when(F.col("hour_24").between(15, 17), "Afternoon")
         .when(F.col("hour_24").between(18, 20), "Evening")
         .when(F.col("hour_24").between(21, 23), "Night")
         .otherwise("Late-Night")
    ) \
    .withColumn("business_time_period",
        F.when(F.col("hour_24").between(7, 9), "Rush-Hour-Morning")
         .when(F.col("hour_24").between(10, 16), "Business-Hours")
         .when(F.col("hour_24").between(17, 19), "Rush-Hour-Evening")
         .when(F.col("hour_24").between(20, 22), "Prime-Time")
         .otherwise("Off-Peak")
    ) \
    .withColumn("is_business_hours", F.col("hour_24").between(8, 17)) \
    .withColumn("shift_category",
        F.when(F.col("hour_24").between(6, 14), "Morning")
         .when(F.col("hour_24").between(14, 22), "Evening")
         .otherwise("Night")
    )

time_df.write.mode("overwrite").saveAsTable("silver.dim_time")

# Print dimension counts
dim_counts = {
    "stores": dim_store.count(),
    "brands": dim_brand.count(),
    "categories": dim_category.count(),
    "dates": date_df.count() if 'date_df' in locals() else 0,
    "time": time_df.count()
}

print(f"üìä Dimension tables created:")
for table, count in dim_counts.items():
    print(f"  - dim_{table}: {count:,} rows")

# =====================================================
# DATA QUALITY VALIDATION
# =====================================================

print("üîç Performing data quality validation...")

# Validate single date authority
date_validation = tx.filter(F.col("transaction_date").isNull()).count()
if date_validation == 0:
    print("‚úÖ Single date authority enforced: All transactions have transaction_date")
else:
    print(f"‚ùå Date authority violation: {date_validation} transactions missing transaction_date")

# Validate transaction value consistency
revenue_validation = tx.agg(
    F.sum("transaction_value").alias("total_revenue"),
    F.count("*").alias("transaction_count"),
    F.avg("transaction_value").alias("avg_transaction_value")
).collect()[0]

print(f"üí∞ Revenue validation:")
print(f"  - Total revenue: ‚Ç±{revenue_validation['total_revenue']:,.2f}")
print(f"  - Average transaction: ‚Ç±{revenue_validation['avg_transaction_value']:.2f}")

# Validate items explosion
items_validation = items_with_nielsen.agg(
    F.sum("item_total").alias("total_item_value"),
    F.count("*").alias("total_items")
).collect()[0]

print(f"üì¶ Items validation:")
print(f"  - Total item value: ‚Ç±{items_validation['total_item_value']:,.2f}")
print(f"  - Total items: {items_validation['total_items']:,}")

# =====================================================
# SAVE PROCESSING METADATA
# =====================================================

processing_metadata = {
    "processing_id": datetime.now().strftime("%Y%m%d_%H%M%S"),
    "processing_timestamp": datetime.now().isoformat(),
    "layer": "silver",
    "source_system": "bronze_lakehouse",
    "tables_created": {
        "silver.transactions": tx_count,
        "silver.transaction_items": items_count,
        "silver.dim_store": dim_counts["stores"],
        "silver.dim_brand": dim_counts["brands"],
        "silver.dim_category": dim_counts["categories"],
        "silver.dim_date": dim_counts["dates"],
        "silver.dim_time": dim_counts["time"]
    },
    "data_quality": {
        "single_date_authority": date_validation == 0,
        "total_revenue": float(revenue_validation['total_revenue']),
        "total_transactions": tx_count,
        "total_items": items_count
    },
    "status": "completed"
}

metadata_df = spark.createDataFrame([processing_metadata])
metadata_df.write.mode("append").saveAsTable("silver.processing_metadata")

print("")
print("=" * 50)
print("üéâ Silver transformation completed successfully!")
print("=" * 50)
print(f"üìä Summary:")
print(f"  - Transactions: {tx_count:,}")
print(f"  - Transaction Items: {items_count:,}")
print(f"  - Dimensions: {sum(dim_counts.values()):,}")
print(f"  - Single date authority: ‚úÖ Enforced")
print(f"‚è∞ Completed at: {datetime.now()}")
print("")
print("Next step: Execute Gold aggregations or connect Warehouse views")
print("=" * 50)