{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver Layer Transformation\n",
    "## Microsoft Fabric Lakehouse - Scout v7 Data Transformation\n",
    "\n",
    "This notebook transforms Bronze data into the Silver layer with:\n",
    "- JSON payload parsing and item explosion\n",
    "- Single authoritative date enforcement\n",
    "- Data quality validation and cleansing\n",
    "- Dimensional modeling\n",
    "\n",
    "**Source**: Bronze Lakehouse tables  \n",
    "**Target**: Silver Lakehouse tables (Delta format)  \n",
    "**Pattern**: Structured transformation with quality gates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Setup\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import re\n",
    "\n",
    "print(f\"Starting Silver transformation at: {datetime.now()}\")\n",
    "print(\"üìä Single Date Authority: transaction_date from SalesInteractionFact ONLY\")\n",
    "print(\"üîß JSON Parsing: PayloadTransactions.payload_json ‚Üí item-level records\")\n",
    "print(\"‚úÖ Quality Gates: Data validation and cleansing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Bronze Data\n",
    "print(\"üì• Loading Bronze data...\")\n",
    "\n",
    "# Sales Interactions (authoritative for dates and demographics)\n",
    "si_bronze = spark.table(\"bronze.sales_interactions_raw\")\n",
    "si_count = si_bronze.count()\n",
    "print(f\"üìä Sales Interactions: {si_count:,} rows\")\n",
    "\n",
    "# Payload Transactions (authoritative for transaction details and items)\n",
    "try:\n",
    "    pt_bronze = spark.table(\"bronze.payload_transactions_raw\")\n",
    "    pt_count = pt_bronze.count()\n",
    "    print(f\"üìä Payload Transactions: {pt_count:,} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è PayloadTransactions not available: {str(e)}\")\n",
    "    pt_bronze = None\n",
    "    pt_count = 0\n",
    "\n",
    "# Reference data\n",
    "stores_bronze = spark.table(\"bronze.stores_raw\") if spark.catalog.tableExists(\"bronze.stores_raw\") else None\n",
    "brands_bronze = spark.table(\"bronze.brands_raw\") if spark.catalog.tableExists(\"bronze.brands_raw\") else None\n",
    "categories_bronze = spark.table(\"bronze.categories_raw\") if spark.catalog.tableExists(\"bronze.categories_raw\") else None\n",
    "\n",
    "print(f\"üìä Reference tables loaded: Stores={stores_bronze is not None}, Brands={brands_bronze is not None}, Categories={categories_bronze is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality and Cleansing\n",
    "print(\"üßπ Data quality and cleansing...\")\n",
    "\n",
    "# Normalize canonical_tx_id (critical for joins)\n",
    "si_clean = si_bronze.withColumn(\n",
    "    \"canonical_tx_id\", \n",
    "    F.lower(F.trim(F.col(\"canonical_tx_id\")))\n",
    ").filter(\n",
    "    F.col(\"canonical_tx_id\").isNotNull() & \n",
    "    (F.length(F.col(\"canonical_tx_id\")) > 0)\n",
    ")\n",
    "\n",
    "if pt_bronze is not None:\n",
    "    pt_clean = pt_bronze.withColumn(\n",
    "        \"canonical_tx_id\", \n",
    "        F.lower(F.trim(F.col(\"canonical_tx_id\")))\n",
    "    ).filter(\n",
    "        F.col(\"canonical_tx_id\").isNotNull() & \n",
    "        (F.length(F.col(\"canonical_tx_id\")) > 0)\n",
    "    )\n",
    "else:\n",
    "    pt_clean = None\n",
    "\n",
    "# Data quality metrics\n",
    "si_clean_count = si_clean.count()\n",
    "si_unique_tx = si_clean.select(\"canonical_tx_id\").distinct().count()\n",
    "\n",
    "print(f\"üìä Sales Interactions after cleaning: {si_clean_count:,} rows ({si_unique_tx:,} unique transactions)\")\n",
    "\n",
    "if pt_clean is not None:\n",
    "    pt_clean_count = pt_clean.count()\n",
    "    pt_unique_tx = pt_clean.select(\"canonical_tx_id\").distinct().count()\n",
    "    print(f\"üìä Payload Transactions after cleaning: {pt_clean_count:,} rows ({pt_unique_tx:,} unique transactions)\")\n",
    "    \n",
    "    # Check transaction overlap\n",
    "    overlap_count = si_clean.join(\n",
    "        pt_clean.select(\"canonical_tx_id\").distinct(), \n",
    "        \"canonical_tx_id\", \n",
    "        \"inner\"\n",
    "    ).count()\n",
    "    print(f\"üìä Transaction overlap: {overlap_count:,} transactions ({overlap_count/si_unique_tx*100:.1f}% coverage)\")\nelse:\n",
    "    print(\"üìä No PayloadTransactions available - using SalesInteractions only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse JSON Items from PayloadTransactions\n",
    "print(\"üîß Parsing JSON items from PayloadTransactions...\")\n",
    "\n",
    "if pt_clean is not None and pt_clean.filter(F.col(\"payload_json\").isNotNull()).count() > 0:\n",
    "    # Sample JSON to understand structure\n",
    "    sample_json = pt_clean.filter(\n",
    "        F.col(\"payload_json\").isNotNull() & \n",
    "        (F.length(F.col(\"payload_json\")) > 50)\n",
    "    ).select(\"payload_json\").first()\n",
    "    \n",
    "    if sample_json:\n",
    "        print(f\"üìÑ Sample JSON structure: {sample_json['payload_json'][:200]}...\")\n",
    "    \n",
    "    # Extract items using get_json_object and explode\n",
    "    items_df = (\n",
    "        pt_clean\n",
    "        .filter(F.col(\"payload_json\").isNotNull())\n",
    "        .withColumn(\n",
    "            \"items_json\", \n",
    "            F.get_json_object(\"payload_json\", \"$.items\")\n",
    "        )\n",
    "        .filter(F.col(\"items_json\").isNotNull())\n",
    "        .withColumn(\n",
    "            \"items_array\", \n",
    "            F.from_json(\n",
    "                F.col(\"items_json\"), \n",
    "                ArrayType(MapType(StringType(), StringType()))\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\"item\", F.explode_outer(\"items_array\"))\n",
    "        .select(\n",
    "            F.col(\"canonical_tx_id\"),\n",
    "            F.col(\"storeId\").alias(\"store_id\"),\n",
    "            F.col(\"amount\").cast(\"decimal(18,2)\").alias(\"transaction_total\"),\n",
    "            \n",
    "            # Customer data from JSON\n",
    "            F.get_json_object(\"payload_json\", \"$.customer.facialId\").alias(\"facial_id\"),\n",
    "            F.get_json_object(\"payload_json\", \"$.customer.age\").cast(\"int\").alias(\"customer_age_json\"),\n",
    "            F.get_json_object(\"payload_json\", \"$.customer.gender\").alias(\"customer_gender_json\"),\n",
    "            F.get_json_object(\"payload_json\", \"$.timestamp\").alias(\"event_ts\"),\n",
    "            \n",
    "            # Item details\n",
    "            F.col(\"item\")[\"sku\"].alias(\"sku\"),\n",
    "            F.col(\"item\")[\"brand\"].alias(\"item_brand\"),\n",
    "            F.col(\"item\")[\"category\"].alias(\"item_category\"),\n",
    "            F.col(\"item\")[\"quantity\"].cast(\"int\").alias(\"item_qty\"),\n",
    "            F.col(\"item\")[\"unitPrice\"].cast(\"decimal(18,2)\").alias(\"item_unit_price\"),\n",
    "            F.col(\"item\")[\"total\"].cast(\"decimal(18,2)\").alias(\"item_total\"),\n",
    "            F.col(\"item\")[\"substitution\"].cast(\"boolean\").alias(\"is_substitution\"),\n",
    "            F.col(\"item\")[\"originalSku\"].alias(\"original_sku\"),\n",
    "            F.col(\"item\")[\"substitutionReason\"].alias(\"substitution_reason\")\n",
    "        )\n",
    "        .withColumn(\"item_sequence\", F.row_number().over(\n",
    "            Window.partitionBy(\"canonical_tx_id\").orderBy(F.col(\"item_total\").desc())\n",
    "        ))\n",
    "    )\n",
    "    \n",
    "    items_count = items_df.count()\n",
    "    unique_skus = items_df.select(\"sku\").distinct().count()\n",
    "    \n",
    "    print(f\"üìä Transaction items parsed: {items_count:,} items\")\n",
    "    print(f\"üìä Unique SKUs: {unique_skus:,}\")\n",
    "    \n",
    "    # Write to Silver\n",
    "    items_df.write.mode(\"overwrite\").saveAsTable(\"silver.transaction_items\")\n",
    "    print(\"‚úÖ Transaction items written to silver.transaction_items\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No valid JSON payloads found - creating empty items table\")\n",
    "    \n",
    "    # Create empty schema for consistency\n",
    "    empty_items_schema = StructType([\n",
    "        StructField(\"canonical_tx_id\", StringType(), True),\n",
    "        StructField(\"store_id\", IntegerType(), True),\n",
    "        StructField(\"item_sequence\", IntegerType(), True),\n",
    "        StructField(\"sku\", StringType(), True),\n",
    "        StructField(\"item_brand\", StringType(), True),\n",
    "        StructField(\"item_category\", StringType(), True),\n",
    "        StructField(\"item_qty\", IntegerType(), True),\n",
    "        StructField(\"item_unit_price\", DecimalType(18,2), True),\n",
    "        StructField(\"item_total\", DecimalType(18,2), True),\n",
    "        StructField(\"is_substitution\", BooleanType(), True),\n",
    "        StructField(\"original_sku\", StringType(), True),\n",
    "        StructField(\"substitution_reason\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    empty_items_df = spark.createDataFrame([], empty_items_schema)\n",
    "    empty_items_df.write.mode(\"overwrite\").saveAsTable(\"silver.transaction_items\")\n",
    "    print(\"‚úÖ Empty transaction items schema created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Silver Transactions (Single Date Authority)\n",
    "print(\"üèóÔ∏è Creating Silver transactions with single date authority...\")\n",
    "\n",
    "# Start with SalesInteractionFact as the authoritative source\n",
    "transactions_silver = (\n",
    "    si_clean\n",
    "    .select(\n",
    "        F.col(\"canonical_tx_id\"),\n",
    "        F.col(\"interaction_id\"),\n",
    "        F.col(\"store_id\"),\n",
    "        F.col(\"customer_id\").alias(\"customer_id\"),  # facial_id\n",
    "        \n",
    "        # SINGLE AUTHORITATIVE DATE SOURCE\n",
    "        F.col(\"transaction_date\").cast(\"date\").alias(\"transaction_date\"),\n",
    "        F.col(\"transaction_time\").alias(\"transaction_time\"),\n",
    "        F.col(\"created_date\").cast(\"timestamp\").alias(\"created_ts\"),\n",
    "        \n",
    "        # Dimensional keys\n",
    "        F.col(\"date_key\"),\n",
    "        F.col(\"time_key\"),\n",
    "        \n",
    "        # Customer demographics\n",
    "        F.col(\"age\").cast(\"int\").alias(\"customer_age\"),\n",
    "        F.col(\"gender\").alias(\"customer_gender\"),\n",
    "        F.col(\"emotional_state\"),\n",
    "        F.col(\"transcription_text\").alias(\"transcript_text\"),\n",
    "        \n",
    "        # Device and persona\n",
    "        F.col(\"device_id\"),\n",
    "        F.col(\"assigned_persona\").alias(\"persona_assigned\"),\n",
    "        F.col(\"persona_rule_id\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add transaction value and basket size from PayloadTransactions if available\n",
    "if pt_clean is not None:\n",
    "    pt_aggregated = (\n",
    "        pt_clean\n",
    "        .groupBy(\"canonical_tx_id\")\n",
    "        .agg(\n",
    "            F.first(\"amount\").alias(\"transaction_value\"),\n",
    "            F.count(\"*\").alias(\"basket_size\"),\n",
    "            F.first(\"storeId\").alias(\"store_id_json\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    transactions_silver = transactions_silver.join(\n",
    "        pt_aggregated, \"canonical_tx_id\", \"left\"\n",
    "    ).withColumn(\n",
    "        \"store_id\", \n",
    "        F.coalesce(F.col(\"store_id\"), F.col(\"store_id_json\"))\n",
    "    ).drop(\"store_id_json\")\nelse:\n",
    "    # Use placeholder values if no PayloadTransactions\n",
    "    transactions_silver = transactions_silver.withColumn(\n",
    "        \"transaction_value\", F.lit(None).cast(\"decimal(18,2)\")\n",
    "    ).withColumn(\n",
    "        \"basket_size\", F.lit(1)\n",
    "    )\n",
    "\n",
    "# Add time-based derived fields\n",
    "transactions_silver = (\n",
    "    transactions_silver\n",
    "    .withColumn(\"hour_24\", F.hour(\"created_ts\"))\n",
    "    .withColumn(\n",
    "        \"weekday_vs_weekend\", \n",
    "        F.when(F.dayofweek(\"transaction_date\").isin(1, 7), \"Weekend\")\n",
    "         .otherwise(\"Weekday\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"time_of_day_category\",\n",
    "        F.when(F.col(\"hour_24\").between(6, 8), \"Early-Morning\")\n",
    "         .when(F.col(\"hour_24\").between(9, 11), \"Late-Morning\")\n",
    "         .when(F.col(\"hour_24\").between(12, 14), \"Lunch-Time\")\n",
    "         .when(F.col(\"hour_24\").between(15, 17), \"Afternoon\")\n",
    "         .when(F.col(\"hour_24\").between(18, 20), \"Evening\")\n",
    "         .when(F.col(\"hour_24\").between(21, 23), \"Night\")\n",
    "         .otherwise(\"Late-Night\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"business_time_period\",\n",
    "        F.when(F.col(\"hour_24\").between(7, 9), \"Rush-Hour-Morning\")\n",
    "         .when(F.col(\"hour_24\").between(10, 16), \"Business-Hours\")\n",
    "         .when(F.col(\"hour_24\").between(17, 19), \"Rush-Hour-Evening\")\n",
    "         .when(F.col(\"hour_24\").between(20, 22), \"Prime-Time\")\n",
    "         .otherwise(\"Off-Peak\")\n",
    "    )\n",
    "    .withColumn(\"was_substitution\", F.lit(False))  # Default, will be updated from items\n",
    "    .withColumn(\"conversation_score\", F.lit(None).cast(\"decimal(5,2)\"))\n",
    "    .withColumn(\"persona_confidence\", F.lit(None).cast(\"decimal(5,2)\"))\n",
    ")\n",
    "\n",
    "# Data quality validation\n",
    "tx_count = transactions_silver.count()\n",
    "tx_with_dates = transactions_silver.filter(F.col(\"transaction_date\").isNotNull()).count()\n",
    "tx_date_range = transactions_silver.agg(\n",
    "    F.min(\"transaction_date\").alias(\"min_date\"),\n",
    "    F.max(\"transaction_date\").alias(\"max_date\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"üìä Silver transactions created: {tx_count:,} rows\")\n",
    "print(f\"üìä Transactions with valid dates: {tx_with_dates:,} ({tx_with_dates/tx_count*100:.1f}%)\")\n",
    "print(f\"üìä Date range: {tx_date_range['min_date']} to {tx_date_range['max_date']}\")\n",
    "\n",
    "# Write to Silver\n",
    "transactions_silver.write.mode(\"overwrite\").saveAsTable(\"silver.transactions\")\n",
    "print(\"‚úÖ Transactions written to silver.transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Silver Dimensions\n",
    "print(\"üèóÔ∏è Creating Silver dimensions...\")\n",
    "\n",
    "# Store Dimension\n",
    "if stores_bronze is not None:\n",
    "    stores_silver = (\n",
    "        stores_bronze\n",
    "        .select(\n",
    "            F.col(\"StoreID\").alias(\"store_id\"),\n",
    "            F.col(\"StoreName\").alias(\"store_name\"),\n",
    "            F.coalesce(F.col(\"Region\"), F.lit(\"Unknown\")).alias(\"region_name\"),\n",
    "            F.coalesce(F.col(\"Province\"), F.lit(\"Unknown\")).alias(\"province_name\"),\n",
    "            F.coalesce(F.col(\"Municipality\"), F.lit(\"Unknown\")).alias(\"municipality_name\"),\n",
    "            F.coalesce(F.col(\"Barangay\"), F.lit(\"Unknown\")).alias(\"barangay_name\"),\n",
    "            F.coalesce(F.col(\"StoreType\"), F.lit(\"General\")).alias(\"store_type\"),\n",
    "            F.col(\"Latitude\").cast(\"decimal(10,6)\").alias(\"latitude\"),\n",
    "            F.col(\"Longitude\").cast(\"decimal(10,6)\").alias(\"longitude\"),\n",
    "            F.lit(True).alias(\"is_active\"),\n",
    "            F.current_timestamp().alias(\"created_date\")\n",
    "        )\n",
    "        .distinct()\n",
    "    )\n",
    "    \n",
    "    stores_count = stores_silver.count()\n",
    "    print(f\"üìä Store dimension: {stores_count:,} stores\")\n",
    "    \n",
    "    stores_silver.write.mode(\"overwrite\").saveAsTable(\"silver.dim_store\")\n",
    "    print(\"‚úÖ Store dimension written to silver.dim_store\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No stores data available - creating minimal dimension from transactions\")\n",
    "    \n",
    "    # Create minimal store dimension from transaction data\n",
    "    stores_silver = (\n",
    "        transactions_silver\n",
    "        .select(\"store_id\")\n",
    "        .distinct()\n",
    "        .filter(F.col(\"store_id\").isNotNull())\n",
    "        .withColumn(\"store_name\", F.concat(F.lit(\"Store \"), F.col(\"store_id\")))\n",
    "        .withColumn(\"region_name\", F.lit(\"Unknown\"))\n",
    "        .withColumn(\"province_name\", F.lit(\"Unknown\"))\n",
    "        .withColumn(\"municipality_name\", F.lit(\"Unknown\"))\n",
    "        .withColumn(\"barangay_name\", F.lit(\"Unknown\"))\n",
    "        .withColumn(\"store_type\", F.lit(\"General\"))\n",
    "        .withColumn(\"latitude\", F.lit(None).cast(\"decimal(10,6)\"))\n",
    "        .withColumn(\"longitude\", F.lit(None).cast(\"decimal(10,6)\"))\n",
    "        .withColumn(\"is_active\", F.lit(True))\n",
    "        .withColumn(\"created_date\", F.current_timestamp())\n",
    "    )\n",
    "    \n",
    "    stores_silver.write.mode(\"overwrite\").saveAsTable(\"silver.dim_store\")\n",
    "    print(\"‚úÖ Minimal store dimension created\")\n",
    "\n",
    "# Brand Dimension\n",
    "if brands_bronze is not None:\n",
    "    brands_silver = (\n",
    "        brands_bronze\n",
    "        .select(\n",
    "            F.monotonically_increasing_id().alias(\"brand_id\"),\n",
    "            F.col(\"BrandName\").alias(\"brand_name\"),\n",
    "            F.coalesce(F.col(\"Category\"), F.lit(\"General\")).alias(\"brand_category\"),\n",
    "            F.lit(None).alias(\"nielsen_l1_category\"),\n",
    "            F.lit(None).alias(\"nielsen_l2_category\"),\n",
    "            F.lit(None).alias(\"nielsen_l3_category\"),\n",
    "            F.lit(False).alias(\"is_premium\"),\n",
    "            F.lit(\"Mass Market\").alias(\"market_segment\"),\n",
    "            F.current_timestamp().alias(\"created_date\")\n",
    "        )\n",
    "        .distinct()\n",
    "    )\n",
    "    \n",
    "    brands_count = brands_silver.count()\n",
    "    print(f\"üìä Brand dimension: {brands_count:,} brands\")\n",
    "    \n",
    "    brands_silver.write.mode(\"overwrite\").saveAsTable(\"silver.dim_brand\")\n",
    "    print(\"‚úÖ Brand dimension written to silver.dim_brand\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Creating brand dimension from transaction items...\")\n",
    "    \n",
    "    # Create from transaction items if available\n",
    "    if spark.catalog.tableExists(\"silver.transaction_items\"):\n",
    "        items_for_brands = spark.table(\"silver.transaction_items\")\n",
    "        if items_for_brands.count() > 0:\n",
    "            brands_silver = (\n",
    "                items_for_brands\n",
    "                .select(\"item_brand\")\n",
    "                .distinct()\n",
    "                .filter(F.col(\"item_brand\").isNotNull())\n",
    "                .withColumn(\"brand_id\", F.monotonically_increasing_id())\n",
    "                .withColumn(\"brand_name\", F.col(\"item_brand\"))\n",
    "                .withColumn(\"brand_category\", F.lit(\"General\"))\n",
    "                .withColumn(\"nielsen_l1_category\", F.lit(None))\n",
    "                .withColumn(\"nielsen_l2_category\", F.lit(None))\n",
    "                .withColumn(\"nielsen_l3_category\", F.lit(None))\n",
    "                .withColumn(\"is_premium\", F.lit(False))\n",
    "                .withColumn(\"market_segment\", F.lit(\"Mass Market\"))\n",
    "                .withColumn(\"created_date\", F.current_timestamp())\n",
    "                .drop(\"item_brand\")\n",
    "            )\n",
    "        else:\n",
    "            brands_silver = spark.createDataFrame([], StructType([\n",
    "                StructField(\"brand_id\", LongType(), True),\n",
    "                StructField(\"brand_name\", StringType(), True),\n",
    "                StructField(\"brand_category\", StringType(), True),\n",
    "                StructField(\"nielsen_l1_category\", StringType(), True),\n",
    "                StructField(\"nielsen_l2_category\", StringType(), True),\n",
    "                StructField(\"nielsen_l3_category\", StringType(), True),\n",
    "                StructField(\"is_premium\", BooleanType(), True),\n",
    "                StructField(\"market_segment\", StringType(), True),\n",
    "                StructField(\"created_date\", TimestampType(), True)\n",
    "            ]))\n",
    "    else:\n",
    "        brands_silver = spark.createDataFrame([], StructType())\n",
    "    \n",
    "    brands_silver.write.mode(\"overwrite\").saveAsTable(\"silver.dim_brand\")\n",
    "    print(\"‚úÖ Brand dimension created from items\")\n",
    "\n",
    "# Category Dimension (similar pattern)\n",
    "categories_silver = spark.createDataFrame([], StructType([\n",
    "    StructField(\"category_id\", LongType(), True),\n",
    "    StructField(\"category_name\", StringType(), True),\n",
    "    StructField(\"parent_category_id\", LongType(), True),\n",
    "    StructField(\"category_level\", IntegerType(), True),\n",
    "    StructField(\"nielsen_mapping\", StringType(), True),\n",
    "    StructField(\"is_tobacco\", BooleanType(), True),\n",
    "    StructField(\"is_laundry\", BooleanType(), True),\n",
    "    StructField(\"created_date\", TimestampType(), True)\n",
    "]))\n",
    "\n",
    "categories_silver.write.mode(\"overwrite\").saveAsTable(\"silver.dim_category\")\n",
    "print(\"‚úÖ Category dimension (empty schema) created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Date and Time Dimensions\n",
    "print(\"üìÖ Creating date and time dimensions...\")\n",
    "\n",
    "# Date dimension\n",
    "date_range = transactions_silver.agg(\n",
    "    F.min(\"transaction_date\").alias(\"min_date\"),\n",
    "    F.max(\"transaction_date\").alias(\"max_date\")\n",
    ").collect()[0]\n",
    "\n",
    "if date_range['min_date'] and date_range['max_date']:\n",
    "    # Generate date range\n",
    "    from datetime import datetime, timedelta\n",
    "    import calendar\n",
    "    \n",
    "    start_date = date_range['min_date']\n",
    "    end_date = date_range['max_date']\n",
    "    \n",
    "    # Extend range slightly\n",
    "    start_date = start_date - timedelta(days=30)\n",
    "    end_date = end_date + timedelta(days=30)\n",
    "    \n",
    "    # Create date records\n",
    "    date_records = []\n",
    "    current_date = start_date\n",
    "    \n",
    "    while current_date <= end_date:\n",
    "        date_key = int(current_date.strftime(\"%Y%m%d\"))\n",
    "        \n",
    "        # Check if it's payday period (15th or 30th +/- 2 days)\n",
    "        is_payday = (\n",
    "            (13 <= current_date.day <= 17) or  # Mid-month payday\n",
    "            (current_date.day >= 28) or       # End-month payday\n",
    "            (current_date.day <= 2)           # Early next month\n",
    "        )\n",
    "        \n",
    "        # Salary week (1st and 3rd week of month)\n",
    "        week_of_month = (current_date.day - 1) // 7 + 1\n",
    "        salary_week = week_of_month in [1, 3]\n",
    "        \n",
    "        date_records.append((\n",
    "            date_key,\n",
    "            current_date,\n",
    "            current_date.weekday() + 1,  # 1=Monday, 7=Sunday\n",
    "            calendar.day_name[current_date.weekday()],\n",
    "            current_date.day,\n",
    "            current_date.timetuple().tm_yday,\n",
    "            current_date.isocalendar()[1],\n",
    "            current_date.month,\n",
    "            calendar.month_name[current_date.month],\n",
    "            (current_date.month - 1) // 3 + 1,\n",
    "            current_date.year,\n",
    "            current_date.weekday() >= 5,  # Weekend\n",
    "            is_payday,\n",
    "            salary_week\n",
    "        ))\n",
    "        \n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    date_schema = StructType([\n",
    "        StructField(\"date_key\", IntegerType(), False),\n",
    "        StructField(\"full_date\", DateType(), False),\n",
    "        StructField(\"day_of_week\", IntegerType(), False),\n",
    "        StructField(\"day_name\", StringType(), False),\n",
    "        StructField(\"day_of_month\", IntegerType(), False),\n",
    "        StructField(\"day_of_year\", IntegerType(), False),\n",
    "        StructField(\"week_of_year\", IntegerType(), False),\n",
    "        StructField(\"month_number\", IntegerType(), False),\n",
    "        StructField(\"month_name\", StringType(), False),\n",
    "        StructField(\"quarter\", IntegerType(), False),\n",
    "        StructField(\"year\", IntegerType(), False),\n",
    "        StructField(\"is_weekend\", BooleanType(), False),\n",
    "        StructField(\"is_payday_period\", BooleanType(), False),\n",
    "        StructField(\"salary_week\", BooleanType(), False)\n",
    "    ])\n",
    "    \n",
    "    date_dim_df = spark.createDataFrame(date_records, date_schema)\n",
    "    date_count = date_dim_df.count()\n",
    "    \n",
    "    print(f\"üìä Date dimension: {date_count:,} dates from {start_date} to {end_date}\")\n",
    "    \n",
    "    date_dim_df.write.mode(\"overwrite\").saveAsTable(\"silver.dim_date\")\n",
    "    print(\"‚úÖ Date dimension written to silver.dim_date\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No valid transaction dates found for date dimension\")\n",
    "\n",
    "# Time dimension (24 hours)\n",
    "time_records = []\n",
    "for hour in range(24):\n",
    "    for minute in [0, 15, 30, 45]:  # 15-minute intervals\n",
    "        time_key = hour * 100 + minute\n",
    "        \n",
    "        # Daypart classification\n",
    "        if 6 <= hour < 12:\n",
    "            daypart = \"Morning\"\n",
    "        elif 12 <= hour < 18:\n",
    "            daypart = \"Afternoon\"\n",
    "        elif 18 <= hour < 22:\n",
    "            daypart = \"Evening\"\n",
    "        else:\n",
    "            daypart = \"Night\"\n",
    "        \n",
    "        # Business hours classification\n",
    "        if 9 <= hour < 17:\n",
    "            business_category = \"Business Hours\"\n",
    "        elif 7 <= hour < 9 or 17 <= hour < 19:\n",
    "            business_category = \"Rush Hour\"\n",
    "        else:\n",
    "            business_category = \"Off Hours\"\n",
    "        \n",
    "        time_records.append((\n",
    "            time_key,\n",
    "            f\"{hour:02d}:{minute:02d}:00\",\n",
    "            hour,\n",
    "            hour if hour <= 12 else hour - 12,\n",
    "            minute,\n",
    "            0,  # seconds\n",
    "            \"AM\" if hour < 12 else \"PM\",\n",
    "            daypart,\n",
    "            business_category\n",
    "        ))\n",
    "\n",
    "time_schema = StructType([\n",
    "    StructField(\"time_key\", IntegerType(), False),\n",
    "    StructField(\"time_24h\", StringType(), False),\n",
    "    StructField(\"hour_24\", IntegerType(), False),\n",
    "    StructField(\"hour_12\", IntegerType(), False),\n",
    "    StructField(\"minute\", IntegerType(), False),\n",
    "    StructField(\"second\", IntegerType(), False),\n",
    "    StructField(\"am_pm\", StringType(), False),\n",
    "    StructField(\"daypart\", StringType(), False),\n",
    "    StructField(\"business_hours_category\", StringType(), False)\n",
    "])\n",
    "\n",
    "time_dim_df = spark.createDataFrame(time_records, time_schema)\n",
    "time_count = time_dim_df.count()\n",
    "\n",
    "print(f\"üìä Time dimension: {time_count:,} time periods\")\n",
    "\n",
    "time_dim_df.write.mode(\"overwrite\").saveAsTable(\"silver.dim_time\")\n",
    "print(\"‚úÖ Time dimension written to silver.dim_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver Layer Validation and Summary\n",
    "print(\"üìä Silver Layer Validation Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check all Silver tables\n",
    "silver_tables = [\n",
    "    \"silver.transactions\",\n",
    "    \"silver.transaction_items\",\n",
    "    \"silver.dim_store\",\n",
    "    \"silver.dim_brand\",\n",
    "    \"silver.dim_category\",\n",
    "    \"silver.dim_date\",\n",
    "    \"silver.dim_time\"\n",
    "]\n",
    "\n",
    "silver_summary = []\n",
    "\n",
    "for table in silver_tables:\n",
    "    try:\n",
    "        df = spark.table(table)\n",
    "        row_count = df.count()\n",
    "        col_count = len(df.columns)\n",
    "        \n",
    "        # Additional validation for key tables\n",
    "        validation_notes = []\n",
    "        \n",
    "        if table == \"silver.transactions\":\n",
    "            null_dates = df.filter(F.col(\"transaction_date\").isNull()).count()\n",
    "            if null_dates > 0:\n",
    "                validation_notes.append(f\"{null_dates} null dates\")\n",
    "            \n",
    "            unique_tx = df.select(\"canonical_tx_id\").distinct().count()\n",
    "            if unique_tx != row_count:\n",
    "                validation_notes.append(f\"Duplicates: {row_count - unique_tx}\")\n",
    "        \n",
    "        elif table == \"silver.transaction_items\":\n",
    "            null_skus = df.filter(F.col(\"sku\").isNull()).count()\n",
    "            if null_skus > 0:\n",
    "                validation_notes.append(f\"{null_skus} null SKUs\")\n",
    "        \n",
    "        status = \"‚úÖ Valid\" if len(validation_notes) == 0 else f\"‚ö†Ô∏è Issues: {', '.join(validation_notes)}\"\n",
    "        \n",
    "        silver_summary.append({\n",
    "            \"table\": table,\n",
    "            \"rows\": row_count,\n",
    "            \"columns\": col_count,\n",
    "            \"status\": status\n",
    "        })\n",
    "        \n",
    "        print(f\"{table}: {row_count:,} rows, {col_count} columns - {status}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        silver_summary.append({\n",
    "            \"table\": table,\n",
    "            \"rows\": 0,\n",
    "            \"columns\": 0,\n",
    "            \"status\": f\"‚ùå Error: {str(e)}\"\n",
    "        })\n",
    "        \n",
    "        print(f\"{table}: ‚ùå Error - {str(e)}\")\n",
    "\n",
    "# Data quality summary\n",
    "try:\n",
    "    tx_df = spark.table(\"silver.transactions\")\n",
    "    \n",
    "    quality_metrics = {\n",
    "        \"total_transactions\": tx_df.count(),\n",
    "        \"date_coverage\": tx_df.filter(F.col(\"transaction_date\").isNotNull()).count(),\n",
    "        \"customer_coverage\": tx_df.filter(F.col(\"customer_id\").isNotNull()).count(),\n",
    "        \"store_coverage\": tx_df.filter(F.col(\"store_id\").isNotNull()).count(),\n",
    "        \"unique_customers\": tx_df.select(\"customer_id\").distinct().count(),\n",
    "        \"unique_stores\": tx_df.select(\"store_id\").distinct().count(),\n",
    "        \"date_range_days\": tx_df.agg(\n",
    "            F.datediff(F.max(\"transaction_date\"), F.min(\"transaction_date\")).alias(\"days\")\n",
    "        ).collect()[0][\"days\"] or 0\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüìä Data Quality Metrics:\")\n",
    "    print(f\"Total transactions: {quality_metrics['total_transactions']:,}\")\n",
    "    print(f\"Date coverage: {quality_metrics['date_coverage']:,} ({quality_metrics['date_coverage']/quality_metrics['total_transactions']*100:.1f}%)\")\n",
    "    print(f\"Customer coverage: {quality_metrics['customer_coverage']:,} ({quality_metrics['customer_coverage']/quality_metrics['total_transactions']*100:.1f}%)\")\n",
    "    print(f\"Store coverage: {quality_metrics['store_coverage']:,} ({quality_metrics['store_coverage']/quality_metrics['total_transactions']*100:.1f}%)\")\n",
    "    print(f\"Unique customers: {quality_metrics['unique_customers']:,}\")\n",
    "    print(f\"Unique stores: {quality_metrics['unique_stores']:,}\")\n",
    "    print(f\"Date range: {quality_metrics['date_range_days']:,} days\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not compute quality metrics: {str(e)}\")\n",
    "\n",
    "# Save transformation metadata\n",
    "transformation_metadata = {\n",
    "    \"transformation_id\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    \"transformation_timestamp\": datetime.now().isoformat(),\n",
    "    \"source_layer\": \"bronze\",\n",
    "    \"target_layer\": \"silver\",\n",
    "    \"tables_processed\": silver_summary,\n",
    "    \"single_date_authority\": \"transaction_date from SalesInteractionFact\",\n",
    "    \"json_parsing_completed\": pt_clean is not None and pt_clean.count() > 0,\n",
    "    \"status\": \"completed\"\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_df = spark.createDataFrame([transformation_metadata])\n",
    "metadata_df.write.mode(\"append\").saveAsTable(\"silver.transformation_metadata\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"üéâ Silver transformation completed successfully!\")\n",
    "print(f\"üìä Tables created: {len([s for s in silver_summary if 'Error' not in s['status']])}/{len(silver_tables)}\")\n",
    "print(f\"‚è∞ Completed at: {datetime.now()}\")\n",
    "print(\"\\nNext step: Run 03_gold_aggregations.ipynb\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.8.0"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}