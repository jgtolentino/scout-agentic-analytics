{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bronze Layer Ingestion\n",
    "## Microsoft Fabric Lakehouse - Scout v7 Data Ingestion\n",
    "\n",
    "This notebook ingests data from Azure SQL Database into the Bronze layer of our Lakehouse.\n",
    "\n",
    "**Source**: Azure SQL Database (canonical.SalesInteractionFact, dbo.PayloadTransactions)  \n",
    "**Target**: Lakehouse Bronze tables (Delta format)  \n",
    "**Pattern**: Full load with incremental capability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Setup\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# CONFIGURATION - Update these values\n",
    "AZURE_SQL_SERVER = \"sqltbwaprojectscoutserver.database.windows.net\"\n",
    "AZURE_SQL_DATABASE = \"SQL-TBWA-ProjectScout-Reporting-Prod\"\n",
    "AZURE_SQL_PORT = 1433\n",
    "\n",
    "# Connection will use Managed Identity or connection string from Key Vault\n",
    "# Ensure your Fabric workspace has proper permissions\n",
    "\n",
    "print(f\"Starting Bronze ingestion at: {datetime.now()}\")\n",
    "print(f\"Target server: {AZURE_SQL_SERVER}\")\n",
    "print(f\"Target database: {AZURE_SQL_DATABASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure SQL Connection Configuration\n",
    "# Use Fabric's built-in connector for Azure SQL\n",
    "\n",
    "def get_azure_sql_options(table_name):\n",
    "    \"\"\"Get Azure SQL connection options for JDBC\"\"\"\n",
    "    return {\n",
    "        \"url\": f\"jdbc:sqlserver://{AZURE_SQL_SERVER}:{AZURE_SQL_PORT};database={AZURE_SQL_DATABASE};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\",\n",
    "        \"dbtable\": table_name,\n",
    "        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "        # Authentication will be handled by Fabric's managed identity\n",
    "        \"authentication\": \"ActiveDirectoryMSI\"\n",
    "    }\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    test_df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .options(**get_azure_sql_options(\"(SELECT TOP 1 1 as test)\")) \\\n",
    "        .load()\n",
    "    \n",
    "    test_result = test_df.collect()[0][0]\n",
    "    print(f\"‚úÖ Azure SQL connection successful. Test result: {test_result}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Azure SQL connection failed: {str(e)}\")\n",
    "    print(\"Ensure:\")\n",
    "    print(\"1. Fabric workspace has SQL access permissions\")\n",
    "    print(\"2. Managed Identity is configured for the Lakehouse\")\n",
    "    print(\"3. Network connectivity is available\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest Sales Interactions (canonical.SalesInteractionFact)\n",
    "print(\"üì• Ingesting Sales Interactions from canonical.SalesInteractionFact...\")\n",
    "\n",
    "# Define the schema based on actual table structure\n",
    "sales_interactions_schema = StructType([\n",
    "    StructField(\"interaction_id\", StringType(), True),\n",
    "    StructField(\"store_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"transaction_date\", DateType(), True),\n",
    "    StructField(\"transaction_time\", StringType(), True),\n",
    "    StructField(\"date_key\", IntegerType(), True),\n",
    "    StructField(\"time_key\", IntegerType(), True),\n",
    "    StructField(\"device_id\", StringType(), True),\n",
    "    StructField(\"age\", ByteType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"emotional_state\", StringType(), True),\n",
    "    StructField(\"transcription_text\", StringType(), True),\n",
    "    StructField(\"barangay_id\", IntegerType(), True),\n",
    "    StructField(\"canonical_tx_id_norm\", StringType(), True),\n",
    "    StructField(\"canonical_tx_id\", StringType(), True),\n",
    "    StructField(\"persona_rule_id\", IntegerType(), True),\n",
    "    StructField(\"assigned_persona\", StringType(), True),\n",
    "    StructField(\"created_date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Read from canonical.SalesInteractionFact\n",
    "sales_interactions_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .options(**get_azure_sql_options(\"canonical.SalesInteractionFact\")) \\\n",
    "    .load() \\\n",
    "    .withColumn(\"ingestion_timestamp\", F.current_timestamp()) \\\n",
    "    .withColumn(\"source_system\", F.lit(\"azure_sql\")) \\\n",
    "    .withColumn(\"source_table\", F.lit(\"canonical.SalesInteractionFact\"))\n",
    "\n",
    "# Data quality checks\n",
    "total_rows = sales_interactions_df.count()\n",
    "unique_transactions = sales_interactions_df.select(\"canonical_tx_id\").distinct().count()\n",
    "date_range = sales_interactions_df.agg(\n",
    "    F.min(\"transaction_date\").alias(\"min_date\"),\n",
    "    F.max(\"transaction_date\").alias(\"max_date\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"üìä Sales Interactions loaded: {total_rows:,} rows\")\n",
    "print(f\"üìä Unique transactions: {unique_transactions:,}\")\n",
    "print(f\"üìä Date range: {date_range['min_date']} to {date_range['max_date']}\")\n",
    "\n",
    "# Write to Bronze layer\n",
    "sales_interactions_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"bronze.sales_interactions_raw\")\n",
    "\n",
    "print(\"‚úÖ Sales Interactions written to bronze.sales_interactions_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest Payload Transactions (dbo.PayloadTransactions)\n",
    "print(\"üì• Ingesting Payload Transactions from dbo.PayloadTransactions...\")\n",
    "\n",
    "# Check if PayloadTransactions table exists\n",
    "payload_check_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .options(**get_azure_sql_options(\"(SELECT COUNT(*) as row_count FROM dbo.PayloadTransactions)\")) \\\n",
    "    .load()\n",
    "\n",
    "payload_count = payload_check_df.collect()[0][0]\n",
    "print(f\"üìä PayloadTransactions available: {payload_count:,} rows\")\n",
    "\n",
    "if payload_count > 0:\n",
    "    # Read PayloadTransactions with sample to check schema\n",
    "    payload_sample_df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .options(**get_azure_sql_options(\"(SELECT TOP 10 * FROM dbo.PayloadTransactions)\")) \\\n",
    "        .load()\n",
    "    \n",
    "    print(\"üìä PayloadTransactions schema:\")\n",
    "    payload_sample_df.printSchema()\n",
    "    \n",
    "    # Full load of PayloadTransactions\n",
    "    payload_transactions_df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .options(**get_azure_sql_options(\"dbo.PayloadTransactions\")) \\\n",
    "        .load() \\\n",
    "        .withColumn(\"ingestion_timestamp\", F.current_timestamp()) \\\n",
    "        .withColumn(\"source_system\", F.lit(\"azure_sql\")) \\\n",
    "        .withColumn(\"source_table\", F.lit(\"dbo.PayloadTransactions\"))\n",
    "    \n",
    "    # Normalize canonical_tx_id to lowercase for consistency\n",
    "    payload_transactions_df = payload_transactions_df.withColumn(\n",
    "        \"canonical_tx_id\", \n",
    "        F.lower(F.col(\"canonical_tx_id\"))\n",
    "    )\n",
    "    \n",
    "    # Data quality checks\n",
    "    payload_total_rows = payload_transactions_df.count()\n",
    "    payload_unique_transactions = payload_transactions_df.select(\"canonical_tx_id\").distinct().count()\n",
    "    payload_json_valid = payload_transactions_df.filter(\n",
    "        F.col(\"payload_json\").isNotNull() & \n",
    "        (F.length(F.col(\"payload_json\")) > 10)\n",
    "    ).count()\n",
    "    \n",
    "    print(f\"üìä Payload Transactions loaded: {payload_total_rows:,} rows\")\n",
    "    print(f\"üìä Unique transactions: {payload_unique_transactions:,}\")\n",
    "    print(f\"üìä Valid JSON payloads: {payload_json_valid:,} ({payload_json_valid/payload_total_rows*100:.1f}%)\")\n",
    "    \n",
    "    # Write to Bronze layer\n",
    "    payload_transactions_df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(\"bronze.payload_transactions_raw\")\n",
    "    \n",
    "    print(\"‚úÖ Payload Transactions written to bronze.payload_transactions_raw\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No PayloadTransactions data found. Creating empty table for schema...\")\n",
    "    \n",
    "    # Create empty schema for downstream processing\n",
    "    empty_payload_df = spark.createDataFrame([], StructType([\n",
    "        StructField(\"canonical_tx_id\", StringType(), True),\n",
    "        StructField(\"storeId\", IntegerType(), True),\n",
    "        StructField(\"amount\", DecimalType(18,2), True),\n",
    "        StructField(\"payload_json\", StringType(), True),\n",
    "        StructField(\"ingestion_timestamp\", TimestampType(), True),\n",
    "        StructField(\"source_system\", StringType(), True),\n",
    "        StructField(\"source_table\", StringType(), True)\n",
    "    ]))\n",
    "    \n",
    "    empty_payload_df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(\"bronze.payload_transactions_raw\")\n",
    "    \n",
    "    print(\"‚úÖ Empty PayloadTransactions schema created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest Reference Data (Stores, Brands, Categories)\n",
    "print(\"üì• Ingesting reference data...\")\n",
    "\n",
    "# Stores data\n",
    "try:\n",
    "    stores_df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .options(**get_azure_sql_options(\"dbo.Stores\")) \\\n",
    "        .load() \\\n",
    "        .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "    \n",
    "    stores_count = stores_df.count()\n",
    "    print(f\"üìä Stores loaded: {stores_count:,} rows\")\n",
    "    \n",
    "    stores_df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(\"bronze.stores_raw\")\n",
    "    \n",
    "    print(\"‚úÖ Stores written to bronze.stores_raw\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load Stores table: {str(e)}\")\n",
    "\n",
    "# Brands data\n",
    "try:\n",
    "    brands_df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .options(**get_azure_sql_options(\"dbo.Brands\")) \\\n",
    "        .load() \\\n",
    "        .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "    \n",
    "    brands_count = brands_df.count()\n",
    "    print(f\"üìä Brands loaded: {brands_count:,} rows\")\n",
    "    \n",
    "    brands_df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(\"bronze.brands_raw\")\n",
    "    \n",
    "    print(\"‚úÖ Brands written to bronze.brands_raw\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load Brands table: {str(e)}\")\n",
    "\n",
    "# Categories data\n",
    "try:\n",
    "    categories_df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .options(**get_azure_sql_options(\"dbo.Categories\")) \\\n",
    "        .load() \\\n",
    "        .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "    \n",
    "    categories_count = categories_df.count()\n",
    "    print(f\"üìä Categories loaded: {categories_count:,} rows\")\n",
    "    \n",
    "    categories_df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(\"bronze.categories_raw\")\n",
    "    \n",
    "    print(\"‚úÖ Categories written to bronze.categories_raw\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load Categories table: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Summary and Validation\n",
    "print(\"üìä Bronze Layer Data Quality Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check all Bronze tables\n",
    "bronze_tables = [\n",
    "    \"bronze.sales_interactions_raw\",\n",
    "    \"bronze.payload_transactions_raw\",\n",
    "    \"bronze.stores_raw\",\n",
    "    \"bronze.brands_raw\",\n",
    "    \"bronze.categories_raw\"\n",
    "]\n",
    "\n",
    "bronze_summary = []\n",
    "\n",
    "for table in bronze_tables:\n",
    "    try:\n",
    "        df = spark.table(table)\n",
    "        row_count = df.count()\n",
    "        col_count = len(df.columns)\n",
    "        \n",
    "        bronze_summary.append({\n",
    "            \"table\": table,\n",
    "            \"rows\": row_count,\n",
    "            \"columns\": col_count,\n",
    "            \"status\": \"‚úÖ Success\"\n",
    "        })\n",
    "        \n",
    "        print(f\"{table}: {row_count:,} rows, {col_count} columns\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        bronze_summary.append({\n",
    "            \"table\": table,\n",
    "            \"rows\": 0,\n",
    "            \"columns\": 0,\n",
    "            \"status\": f\"‚ùå Error: {str(e)}\"\n",
    "        })\n",
    "        \n",
    "        print(f\"{table}: ‚ùå Error - {str(e)}\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = spark.createDataFrame(bronze_summary)\n",
    "summary_df.show(truncate=False)\n",
    "\n",
    "# Save ingestion metadata\n",
    "ingestion_metadata = {\n",
    "    \"ingestion_id\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    \"ingestion_timestamp\": datetime.now().isoformat(),\n",
    "    \"source_system\": \"azure_sql\",\n",
    "    \"source_server\": AZURE_SQL_SERVER,\n",
    "    \"source_database\": AZURE_SQL_DATABASE,\n",
    "    \"tables_processed\": bronze_summary,\n",
    "    \"total_rows_ingested\": sum([item[\"rows\"] for item in bronze_summary]),\n",
    "    \"status\": \"completed\"\n",
    "}\n",
    "\n",
    "# Save metadata as JSON\n",
    "metadata_df = spark.createDataFrame([ingestion_metadata])\n",
    "metadata_df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"bronze.ingestion_metadata\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"üéâ Bronze ingestion completed successfully!\")\n",
    "print(f\"üìä Total rows ingested: {ingestion_metadata['total_rows_ingested']:,}\")\n",
    "print(f\"‚è∞ Completed at: {datetime.now()}\")\n",
    "print(\"\\nNext step: Run 02_silver_transformation.ipynb\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}