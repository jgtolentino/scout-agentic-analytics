{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gold Layer Advanced Analytics\n",
        "## Microsoft Fabric Lakehouse - Scout v7 Gold Layer\n",
        "\n",
        "This notebook creates advanced analytics, market basket analysis, and ML features for the Gold layer.\n",
        "\n",
        "**Source**: Silver layer tables (Delta format)  \n",
        "**Target**: Gold analytics tables with ML features  \n",
        "**Pattern**: Advanced aggregations, market basket, persona scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration and Setup\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "\n",
        "print(f\"Starting Gold aggregations at: {datetime.now()}\")\n",
        "print(\"Gold layer: Advanced analytics and ML features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Silver Tables\n",
        "print(\"📥 Loading Silver layer tables...\")\n",
        "\n",
        "# Main fact tables\n",
        "transactions_df = spark.table(\"silver.transactions\")\n",
        "transaction_items_df = spark.table(\"silver.transaction_items\")\n",
        "\n",
        "# Dimensions\n",
        "dim_store_df = spark.table(\"silver.dim_store\")\n",
        "dim_brand_df = spark.table(\"silver.dim_brand\")\n",
        "dim_category_df = spark.table(\"silver.dim_category\")\n",
        "dim_date_df = spark.table(\"silver.dim_date\")\n",
        "dim_time_df = spark.table(\"silver.dim_time\")\n",
        "\n",
        "# Data quality checks\n",
        "total_transactions = transactions_df.count()\n",
        "total_items = transaction_items_df.count()\n",
        "\n",
        "print(f\"📊 Silver data loaded:\")\n",
        "print(f\"  - Transactions: {total_transactions:,}\")\n",
        "print(f\"  - Transaction Items: {total_items:,}\")\n",
        "print(f\"  - Stores: {dim_store_df.count():,}\")\n",
        "print(f\"  - Brands: {dim_brand_df.count():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Nielsen Category Mappings\n",
        "print(\"🏷️ Creating Nielsen category mappings...\")\n",
        "\n",
        "# Enhanced Nielsen mappings with comprehensive coverage\n",
        "nielsen_mappings = {\n",
        "    # Tobacco Products\n",
        "    \"tobacco\": {\n",
        "        \"l1\": \"Tobacco Products\",\n",
        "        \"l2\": \"Cigarettes\",\n",
        "        \"l3\": \"Regular Cigarettes\",\n",
        "        \"is_tobacco\": True,\n",
        "        \"keywords\": [\"cigarette\", \"tobacco\", \"marlboro\", \"lucky strike\", \"philip morris\"]\n",
        "    },\n",
        "    # Laundry & Household\n",
        "    \"laundry\": {\n",
        "        \"l1\": \"Household Care\",\n",
        "        \"l2\": \"Laundry Care\",\n",
        "        \"l3\": \"Fabric Softeners\",\n",
        "        \"is_laundry\": True,\n",
        "        \"keywords\": [\"downy\", \"surf\", \"ariel\", \"tide\", \"fabric\"]\n",
        "    },\n",
        "    # Food & Beverages\n",
        "    \"food\": {\n",
        "        \"l1\": \"Food & Beverages\",\n",
        "        \"l2\": \"Packaged Food\",\n",
        "        \"l3\": \"Snacks\",\n",
        "        \"is_tobacco\": False,\n",
        "        \"keywords\": [\"snack\", \"food\", \"beverage\", \"drink\"]\n",
        "    },\n",
        "    # Personal Care\n",
        "    \"personal_care\": {\n",
        "        \"l1\": \"Health & Beauty\",\n",
        "        \"l2\": \"Personal Care\",\n",
        "        \"l3\": \"Skin Care\",\n",
        "        \"is_tobacco\": False,\n",
        "        \"keywords\": [\"shampoo\", \"soap\", \"toothpaste\", \"cosmetic\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Convert to DataFrame for joins\n",
        "nielsen_rows = []\n",
        "for key, mapping in nielsen_mappings.items():\n",
        "    for keyword in mapping[\"keywords\"]:\n",
        "        nielsen_rows.append({\n",
        "            \"category_key\": key,\n",
        "            \"keyword\": keyword.lower(),\n",
        "            \"nielsen_l1\": mapping[\"l1\"],\n",
        "            \"nielsen_l2\": mapping[\"l2\"],\n",
        "            \"nielsen_l3\": mapping[\"l3\"],\n",
        "            \"is_tobacco\": mapping[\"is_tobacco\"],\n",
        "            \"is_laundry\": mapping.get(\"is_laundry\", False)\n",
        "        })\n",
        "\n",
        "nielsen_df = spark.createDataFrame(nielsen_rows)\n",
        "nielsen_df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"overwriteSchema\", \"true\") \\\n",
        "    .saveAsTable(\"gold.nielsen_mappings\")\n",
        "\n",
        "print(f\"✅ Nielsen mappings created: {len(nielsen_rows)} keyword mappings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Market Basket Analysis\n",
        "print(\"🛒 Performing market basket analysis...\")\n",
        "\n",
        "# Item co-occurrence analysis\n",
        "basket_analysis = transaction_items_df \\\n",
        "    .groupBy(\"canonical_tx_id\") \\\n",
        "    .agg(F.collect_list(\"sku\").alias(\"items_in_basket\")) \\\n",
        "    .withColumn(\"basket_size\", F.size(\"items_in_basket\")) \\\n",
        "    .filter(F.col(\"basket_size\") >= 2)  # Only multi-item baskets\n",
        "\n",
        "# Generate item pairs for association rules\n",
        "from itertools import combinations\n",
        "from pyspark.sql import Row\n",
        "\n",
        "def generate_pairs(items):\n",
        "    \"\"\"Generate all pairs from a list of items\"\"\"\n",
        "    if len(items) < 2:\n",
        "        return []\n",
        "    return [Row(item_a=str(pair[0]), item_b=str(pair[1])) for pair in combinations(sorted(items), 2)]\n",
        "\n",
        "# UDF for pair generation\n",
        "from pyspark.sql.types import ArrayType, StructType, StructField\n",
        "pair_schema = ArrayType(StructType([\n",
        "    StructField(\"item_a\", StringType(), True),\n",
        "    StructField(\"item_b\", StringType(), True)\n",
        "]))\n",
        "\n",
        "generate_pairs_udf = F.udf(generate_pairs, pair_schema)\n",
        "\n",
        "# Generate item pairs and calculate support\n",
        "item_pairs = basket_analysis \\\n",
        "    .withColumn(\"pairs\", generate_pairs_udf(\"items_in_basket\")) \\\n",
        "    .select(\"canonical_tx_id\", F.explode(\"pairs\").alias(\"pair\")) \\\n",
        "    .select(\"canonical_tx_id\", \n",
        "            F.col(\"pair.item_a\").alias(\"item_a\"),\n",
        "            F.col(\"pair.item_b\").alias(\"item_b\"))\n",
        "\n",
        "# Calculate association metrics\n",
        "total_baskets = basket_analysis.count()\n",
        "\n",
        "association_rules = item_pairs \\\n",
        "    .groupBy(\"item_a\", \"item_b\") \\\n",
        "    .agg(F.count(\"*\").alias(\"co_occurrence_count\")) \\\n",
        "    .withColumn(\"support\", F.col(\"co_occurrence_count\") / total_baskets) \\\n",
        "    .filter(F.col(\"support\") >= 0.01)  # Minimum 1% support\n",
        "\n",
        "# Add confidence and lift calculations\n",
        "item_support = transaction_items_df \\\n",
        "    .select(\"sku\", \"canonical_tx_id\") \\\n",
        "    .distinct() \\\n",
        "    .groupBy(\"sku\") \\\n",
        "    .agg(F.count(\"*\").alias(\"item_count\")) \\\n",
        "    .withColumn(\"item_support\", F.col(\"item_count\") / total_baskets)\n",
        "\n",
        "association_rules_final = association_rules \\\n",
        "    .join(item_support.alias(\"a\"), F.col(\"item_a\") == F.col(\"a.sku\")) \\\n",
        "    .join(item_support.alias(\"b\"), F.col(\"item_b\") == F.col(\"b.sku\")) \\\n",
        "    .withColumn(\"confidence\", F.col(\"support\") / F.col(\"a.item_support\")) \\\n",
        "    .withColumn(\"lift\", F.col(\"support\") / (F.col(\"a.item_support\") * F.col(\"b.item_support\"))) \\\n",
        "    .select(\"item_a\", \"item_b\", \"support\", \"confidence\", \"lift\", \"co_occurrence_count\") \\\n",
        "    .filter(F.col(\"lift\") > 1.0)  # Only positive associations\n",
        "\n",
        "# Save market basket results\n",
        "association_rules_final.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"overwriteSchema\", \"true\") \\\n",
        "    .saveAsTable(\"gold.market_basket_rules\")\n",
        "\n",
        "basket_summary = association_rules_final.agg(\n",
        "    F.count(\"*\").alias(\"total_rules\"),\n",
        "    F.max(\"lift\").alias(\"max_lift\"),\n",
        "    F.avg(\"confidence\").alias(\"avg_confidence\")\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"📊 Market basket analysis complete:\")\n",
        "print(f\"  - Association rules: {basket_summary['total_rules']:,}\")\n",
        "print(f\"  - Max lift: {basket_summary['max_lift']:.2f}\")\n",
        "print(f\"  - Avg confidence: {basket_summary['avg_confidence']:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Customer Analytics and Personas\n",
        "print(\"👤 Creating customer analytics and persona features...\")\n",
        "\n",
        "# Customer transaction summary\n",
        "customer_features = transactions_df \\\n",
        "    .filter(F.col(\"facial_id\").isNotNull()) \\\n",
        "    .groupBy(\"facial_id\", \"customer_age\", \"customer_gender\") \\\n",
        "    .agg(\n",
        "        F.count(\"*\").alias(\"transaction_count\"),\n",
        "        F.sum(\"transaction_value\").alias(\"total_spent\"),\n",
        "        F.avg(\"transaction_value\").alias(\"avg_transaction_value\"),\n",
        "        F.avg(\"basket_size\").alias(\"avg_basket_size\"),\n",
        "        F.countDistinct(\"store_id\").alias(\"store_variety\"),\n",
        "        F.min(\"transaction_date\").alias(\"first_visit\"),\n",
        "        F.max(\"transaction_date\").alias(\"last_visit\"),\n",
        "        F.collect_set(\"emotional_state\").alias(\"emotional_states\"),\n",
        "        F.avg(\"conversation_score\").alias(\"avg_conversation_score\")\n",
        "    ) \\\n",
        "    .withColumn(\"customer_lifespan_days\", \n",
        "                F.datediff(\"last_visit\", \"first_visit\")) \\\n",
        "    .withColumn(\"visit_frequency\", \n",
        "                F.when(F.col(\"customer_lifespan_days\") > 0,\n",
        "                      F.col(\"transaction_count\") / F.col(\"customer_lifespan_days\"))\n",
        "                .otherwise(0))\n",
        "\n",
        "# Add spending categories\n",
        "spending_percentiles = customer_features.approxQuantile(\"total_spent\", [0.33, 0.66], 0.01)\n",
        "low_spender_threshold = spending_percentiles[0]\n",
        "high_spender_threshold = spending_percentiles[1]\n",
        "\n",
        "customer_profiles = customer_features \\\n",
        "    .withColumn(\"spending_category\",\n",
        "                F.when(F.col(\"total_spent\") <= low_spender_threshold, \"Low\")\n",
        "                .when(F.col(\"total_spent\") <= high_spender_threshold, \"Medium\")\n",
        "                .otherwise(\"High\")) \\\n",
        "    .withColumn(\"loyalty_score\",\n",
        "                F.when(F.col(\"customer_lifespan_days\") >= 30, 3)\n",
        "                .when(F.col(\"customer_lifespan_days\") >= 7, 2)\n",
        "                .otherwise(1)) \\\n",
        "    .withColumn(\"engagement_score\",\n",
        "                (F.col(\"avg_conversation_score\") * 0.4 +\n",
        "                 F.col(\"visit_frequency\") * 100 * 0.3 +\n",
        "                 F.col(\"store_variety\") * 0.3).cast(\"float\"))\n",
        "\n",
        "# Save customer profiles\n",
        "customer_profiles.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"overwriteSchema\", \"true\") \\\n",
        "    .saveAsTable(\"gold.customer_profiles\")\n",
        "\n",
        "customer_summary = customer_profiles.agg(\n",
        "    F.count(\"*\").alias(\"total_customers\"),\n",
        "    F.avg(\"transaction_count\").alias(\"avg_transactions_per_customer\"),\n",
        "    F.avg(\"total_spent\").alias(\"avg_spend_per_customer\")\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"📊 Customer analytics complete:\")\n",
        "print(f\"  - Unique customers: {customer_summary['total_customers']:,}\")\n",
        "print(f\"  - Avg transactions per customer: {customer_summary['avg_transactions_per_customer']:.1f}\")\n",
        "print(f\"  - Avg spend per customer: {customer_summary['avg_spend_per_customer']:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Store Analytics\n",
        "print(\"🏪 Creating advanced store analytics...\")\n",
        "\n",
        "# Store performance metrics\n",
        "store_performance = transactions_df \\\n",
        "    .join(dim_store_df, \"store_id\") \\\n",
        "    .groupBy(\"store_id\", \"store_name\", \"region_name\", \"province_name\", \n",
        "             \"municipality_name\", \"latitude\", \"longitude\") \\\n",
        "    .agg(\n",
        "        F.count(\"*\").alias(\"transaction_count\"),\n",
        "        F.sum(\"transaction_value\").alias(\"total_revenue\"),\n",
        "        F.avg(\"transaction_value\").alias(\"avg_transaction_value\"),\n",
        "        F.countDistinct(\"facial_id\").alias(\"unique_customers\"),\n",
        "        F.avg(\"basket_size\").alias(\"avg_basket_size\"),\n",
        "        F.countDistinct(\"transaction_date\").alias(\"operating_days\"),\n",
        "        F.avg(\"conversation_score\").alias(\"avg_conversation_score\"),\n",
        "        F.min(\"transaction_date\").alias(\"first_transaction\"),\n",
        "        F.max(\"transaction_date\").alias(\"last_transaction\")\n",
        "    ) \\\n",
        "    .withColumn(\"revenue_per_day\", \n",
        "                F.col(\"total_revenue\") / F.col(\"operating_days\")) \\\n",
        "    .withColumn(\"transactions_per_day\", \n",
        "                F.col(\"transaction_count\") / F.col(\"operating_days\")) \\\n",
        "    .withColumn(\"customer_retention\", \n",
        "                F.col(\"unique_customers\") / F.col(\"transaction_count\"))\n",
        "\n",
        "# Add store performance categories\n",
        "revenue_percentiles = store_performance.approxQuantile(\"total_revenue\", [0.25, 0.5, 0.75], 0.01)\n",
        "\n",
        "store_analytics = store_performance \\\n",
        "    .withColumn(\"performance_tier\",\n",
        "                F.when(F.col(\"total_revenue\") >= revenue_percentiles[2], \"Top\")\n",
        "                .when(F.col(\"total_revenue\") >= revenue_percentiles[1], \"High\")\n",
        "                .when(F.col(\"total_revenue\") >= revenue_percentiles[0], \"Medium\")\n",
        "                .otherwise(\"Low\")) \\\n",
        "    .withColumn(\"efficiency_score\",\n",
        "                (F.col(\"revenue_per_day\") / 1000 * 0.4 +\n",
        "                 F.col(\"avg_conversation_score\") * 0.3 +\n",
        "                 F.col(\"customer_retention\") * 100 * 0.3).cast(\"float\"))\n",
        "\n",
        "# Save store analytics\n",
        "store_analytics.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"overwriteSchema\", \"true\") \\\n",
        "    .saveAsTable(\"gold.store_analytics\")\n",
        "\n",
        "print(f\"✅ Store analytics created for {store_analytics.count():,} stores\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time-based Analytics\n",
        "print(\"⏰ Creating time-based analytics...\")\n",
        "\n",
        "# Hourly patterns\n",
        "hourly_patterns = transactions_df \\\n",
        "    .join(dim_time_df, \"time_key\") \\\n",
        "    .groupBy(\"hour_24\", \"time_of_day_category\", \"business_time_period\") \\\n",
        "    .agg(\n",
        "        F.count(\"*\").alias(\"transaction_count\"),\n",
        "        F.sum(\"transaction_value\").alias(\"total_revenue\"),\n",
        "        F.avg(\"transaction_value\").alias(\"avg_transaction_value\"),\n",
        "        F.countDistinct(\"facial_id\").alias(\"unique_customers\"),\n",
        "        F.avg(\"conversation_score\").alias(\"avg_conversation_score\")\n",
        "    ) \\\n",
        "    .withColumn(\"revenue_per_transaction\", \n",
        "                F.col(\"total_revenue\") / F.col(\"transaction_count\"))\n",
        "\n",
        "# Daily patterns with day of week\n",
        "daily_patterns = transactions_df \\\n",
        "    .join(dim_date_df, \"date_key\") \\\n",
        "    .groupBy(\"day_of_week\", \"day_name\", \"is_weekend\", \"is_holiday\") \\\n",
        "    .agg(\n",
        "        F.count(\"*\").alias(\"transaction_count\"),\n",
        "        F.sum(\"transaction_value\").alias(\"total_revenue\"),\n",
        "        F.avg(\"transaction_value\").alias(\"avg_transaction_value\"),\n",
        "        F.countDistinct(\"facial_id\").alias(\"unique_customers\")\n",
        "    ) \\\n",
        "    .withColumn(\"revenue_index\", \n",
        "                F.col(\"total_revenue\") / F.avg(\"total_revenue\").over(Window.partitionBy()))\n",
        "\n",
        "# Monthly trends\n",
        "monthly_trends = transactions_df \\\n",
        "    .join(dim_date_df, \"date_key\") \\\n",
        "    .groupBy(\"year\", \"month\", \"month_name\", \"quarter\") \\\n",
        "    .agg(\n",
        "        F.count(\"*\").alias(\"transaction_count\"),\n",
        "        F.sum(\"transaction_value\").alias(\"total_revenue\"),\n",
        "        F.countDistinct(\"facial_id\").alias(\"unique_customers\"),\n",
        "        F.countDistinct(\"store_id\").alias(\"active_stores\")\n",
        "    ) \\\n",
        "    .withColumn(\"month_year\", F.concat(F.col(\"year\"), F.lit(\"-\"), \n",
        "                                       F.lpad(F.col(\"month\"), 2, \"0\")))\n",
        "\n",
        "# Save time analytics\n",
        "hourly_patterns.write.mode(\"overwrite\").saveAsTable(\"gold.hourly_patterns\")\n",
        "daily_patterns.write.mode(\"overwrite\").saveAsTable(\"gold.daily_patterns\")\n",
        "monthly_trends.write.mode(\"overwrite\").saveAsTable(\"gold.monthly_trends\")\n",
        "\n",
        "print(f\"✅ Time-based analytics created:\")\n",
        "print(f\"  - Hourly patterns: {hourly_patterns.count()} hours\")\n",
        "print(f\"  - Daily patterns: {daily_patterns.count()} day types\")\n",
        "print(f\"  - Monthly trends: {monthly_trends.count()} months\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Product Analytics with Nielsen Integration\n",
        "print(\"📦 Creating product analytics with Nielsen integration...\")\n",
        "\n",
        "# Enhanced product performance with Nielsen categories\n",
        "product_performance = transaction_items_df \\\n",
        "    .join(transactions_df.select(\"canonical_tx_id\", \"transaction_date\", \"store_id\", \"facial_id\"), \n",
        "          \"canonical_tx_id\") \\\n",
        "    .groupBy(\"sku\", \"item_brand\", \"item_category\", \"nielsen_l1\", \"nielsen_l2\", \"nielsen_l3\") \\\n",
        "    .agg(\n",
        "        F.count(\"*\").alias(\"transaction_frequency\"),\n",
        "        F.sum(\"item_qty\").alias(\"total_quantity_sold\"),\n",
        "        F.sum(\"item_total\").alias(\"total_revenue\"),\n",
        "        F.avg(\"item_unit_price\").alias(\"avg_unit_price\"),\n",
        "        F.countDistinct(\"canonical_tx_id\").alias(\"unique_transactions\"),\n",
        "        F.countDistinct(\"store_id\").alias(\"store_presence\"),\n",
        "        F.countDistinct(\"facial_id\").alias(\"unique_customers\"),\n",
        "        F.sum(F.when(F.col(\"is_substitution\") == True, 1).otherwise(0)).alias(\"substitution_count\")\n",
        "    ) \\\n",
        "    .withColumn(\"avg_quantity_per_transaction\", \n",
        "                F.col(\"total_quantity_sold\") / F.col(\"unique_transactions\")) \\\n",
        "    .withColumn(\"revenue_per_unit\", \n",
        "                F.col(\"total_revenue\") / F.col(\"total_quantity_sold\")) \\\n",
        "    .withColumn(\"substitution_rate\", \n",
        "                F.col(\"substitution_count\") / F.col(\"transaction_frequency\"))\n",
        "\n",
        "# Add performance tiers\n",
        "revenue_percentiles = product_performance.approxQuantile(\"total_revenue\", [0.2, 0.4, 0.6, 0.8], 0.01)\n",
        "\n",
        "product_analytics = product_performance \\\n",
        "    .withColumn(\"performance_tier\",\n",
        "                F.when(F.col(\"total_revenue\") >= revenue_percentiles[3], \"A\")\n",
        "                .when(F.col(\"total_revenue\") >= revenue_percentiles[2], \"B\")\n",
        "                .when(F.col(\"total_revenue\") >= revenue_percentiles[1], \"C\")\n",
        "                .when(F.col(\"total_revenue\") >= revenue_percentiles[0], \"D\")\n",
        "                .otherwise(\"E\")) \\\n",
        "    .withColumn(\"velocity_score\",\n",
        "                (F.col(\"transaction_frequency\") * 0.4 +\n",
        "                 F.col(\"total_quantity_sold\") * 0.3 +\n",
        "                 F.col(\"store_presence\") * 0.3).cast(\"float\"))\n",
        "\n",
        "# Save product analytics\n",
        "product_analytics.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"overwriteSchema\", \"true\") \\\n",
        "    .saveAsTable(\"gold.product_analytics\")\n",
        "\n",
        "print(f\"✅ Product analytics created for {product_analytics.count():,} SKUs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ML Feature Engineering for Predictive Analytics\n",
        "print(\"🤖 Creating ML features for predictive analytics...\")\n",
        "\n",
        "# Customer segmentation features\n",
        "ml_customer_features = customer_profiles \\\n",
        "    .select(\n",
        "        \"facial_id\",\n",
        "        \"customer_age\",\n",
        "        \"transaction_count\",\n",
        "        \"total_spent\",\n",
        "        \"avg_transaction_value\",\n",
        "        \"avg_basket_size\",\n",
        "        \"store_variety\",\n",
        "        \"visit_frequency\",\n",
        "        \"loyalty_score\",\n",
        "        \"engagement_score\"\n",
        "    ) \\\n",
        "    .filter(F.col(\"facial_id\").isNotNull()) \\\n",
        "    .fillna(0)\n",
        "\n",
        "# Prepare features for clustering\n",
        "feature_cols = [\"customer_age\", \"transaction_count\", \"total_spent\", \n",
        "                \"avg_transaction_value\", \"avg_basket_size\", \"store_variety\", \n",
        "                \"visit_frequency\", \"engagement_score\"]\n",
        "\n",
        "# Assemble feature vector\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "ml_features_df = assembler.transform(ml_customer_features)\n",
        "\n",
        "# Scale features\n",
        "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
        "scaler_model = scaler.fit(ml_features_df)\n",
        "scaled_df = scaler_model.transform(ml_features_df)\n",
        "\n",
        "# K-means clustering for customer segmentation\n",
        "kmeans = KMeans(featuresCol=\"scaled_features\", predictionCol=\"cluster\", k=5, seed=42)\n",
        "kmeans_model = kmeans.fit(scaled_df)\n",
        "clustered_df = kmeans_model.transform(scaled_df)\n",
        "\n",
        "# Evaluate clustering\n",
        "evaluator = ClusteringEvaluator(featuresCol=\"scaled_features\", predictionCol=\"cluster\")\n",
        "silhouette_score = evaluator.evaluate(clustered_df)\n",
        "\n",
        "# Add cluster interpretation\n",
        "cluster_summary = clustered_df \\\n",
        "    .groupBy(\"cluster\") \\\n",
        "    .agg(\n",
        "        F.count(\"*\").alias(\"cluster_size\"),\n",
        "        F.avg(\"total_spent\").alias(\"avg_spent\"),\n",
        "        F.avg(\"transaction_count\").alias(\"avg_transactions\"),\n",
        "        F.avg(\"engagement_score\").alias(\"avg_engagement\")\n",
        "    ) \\\n",
        "    .withColumn(\"cluster_label\",\n",
        "                F.when(F.col(\"avg_spent\") > 1000, \"High Value\")\n",
        "                .when(F.col(\"avg_transactions\") > 5, \"Frequent\")\n",
        "                .when(F.col(\"avg_engagement\") > 5, \"Engaged\")\n",
        "                .otherwise(\"Occasional\"))\n",
        "\n",
        "# Join cluster labels back\n",
        "ml_customer_segments = clustered_df \\\n",
        "    .join(cluster_summary.select(\"cluster\", \"cluster_label\"), \"cluster\") \\\n",
        "    .select(\"facial_id\", \"cluster\", \"cluster_label\", *feature_cols)\n",
        "\n",
        "# Save ML features and segments\n",
        "ml_customer_segments.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"overwriteSchema\", \"true\") \\\n",
        "    .saveAsTable(\"gold.customer_segments_ml\")\n",
        "\n",
        "cluster_summary.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"overwriteSchema\", \"true\") \\\n",
        "    .saveAsTable(\"gold.cluster_summary\")\n",
        "\n",
        "print(f\"🤖 ML features created:\")\n",
        "print(f\"  - Customer segments: {ml_customer_segments.count():,} customers\")\n",
        "print(f\"  - Silhouette score: {silhouette_score:.3f}\")\n",
        "print(f\"  - Clusters: {cluster_summary.count()} segments\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Quality Summary and Validation\n",
        "print(\"📊 Gold Layer Data Quality Summary\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check all Gold tables\n",
        "gold_tables = [\n",
        "    \"gold.nielsen_mappings\",\n",
        "    \"gold.market_basket_rules\",\n",
        "    \"gold.customer_profiles\",\n",
        "    \"gold.store_analytics\",\n",
        "    \"gold.hourly_patterns\",\n",
        "    \"gold.daily_patterns\",\n",
        "    \"gold.monthly_trends\",\n",
        "    \"gold.product_analytics\",\n",
        "    \"gold.customer_segments_ml\",\n",
        "    \"gold.cluster_summary\"\n",
        "]\n",
        "\n",
        "gold_summary = []\n",
        "\n",
        "for table in gold_tables:\n",
        "    try:\n",
        "        df = spark.table(table)\n",
        "        row_count = df.count()\n",
        "        col_count = len(df.columns)\n",
        "        \n",
        "        gold_summary.append({\n",
        "            \"table\": table,\n",
        "            \"rows\": row_count,\n",
        "            \"columns\": col_count,\n",
        "            \"status\": \"✅ Success\"\n",
        "        })\n",
        "        \n",
        "        print(f\"{table}: {row_count:,} rows, {col_count} columns\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        gold_summary.append({\n",
        "            \"table\": table,\n",
        "            \"rows\": 0,\n",
        "            \"columns\": 0,\n",
        "            \"status\": f\"❌ Error: {str(e)}\"\n",
        "        })\n",
        "        \n",
        "        print(f\"{table}: ❌ Error - {str(e)}\")\n",
        "\n",
        "# Create summary DataFrame\n",
        "summary_df = spark.createDataFrame(gold_summary)\n",
        "summary_df.show(truncate=False)\n",
        "\n",
        "# Save Gold layer metadata\n",
        "gold_metadata = {\n",
        "    \"processing_id\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
        "    \"processing_timestamp\": datetime.now().isoformat(),\n",
        "    \"layer\": \"gold\",\n",
        "    \"tables_processed\": gold_summary,\n",
        "    \"total_rows_created\": sum([item[\"rows\"] for item in gold_summary]),\n",
        "    \"ml_features\": {\n",
        "        \"customer_segmentation\": True,\n",
        "        \"market_basket_analysis\": True,\n",
        "        \"nielsen_integration\": True,\n",
        "        \"time_series_features\": True\n",
        "    },\n",
        "    \"analytics_capabilities\": {\n",
        "        \"customer_analytics\": True,\n",
        "        \"store_performance\": True,\n",
        "        \"product_intelligence\": True,\n",
        "        \"temporal_patterns\": True\n",
        "    },\n",
        "    \"status\": \"completed\"\n",
        "}\n",
        "\n",
        "# Save metadata\n",
        "metadata_df = spark.createDataFrame([gold_metadata])\n",
        "metadata_df.write \\\n",
        "    .mode(\"append\") \\\n",
        "    .saveAsTable(\"gold.processing_metadata\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(f\"🎉 Gold layer processing completed successfully!\")\n",
        "print(f\"📊 Total rows created: {gold_metadata['total_rows_created']:,}\")\n",
        "print(f\"🤖 ML features: Customer segmentation, Market basket, Nielsen integration\")\n",
        "print(f\"📈 Analytics: Customer, Store, Product, Time-series\")\n",
        "print(f\"⏰ Completed at: {datetime.now()}\")\n",
        "print(\"\\nNext step: Use Gold tables in Warehouse for Power BI\")\n",
        "print(\"=\" * 50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}