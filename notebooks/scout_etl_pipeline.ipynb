{
 "cells": [
  {
   "cell_type": "code",
   "source": "def export_complete_dataset_no_filters():\n    \"\"\"Export complete dataset - ALL rows with canonical transaction IDs, no date filters\"\"\"\n    \n    print(\"ðŸš€ Exporting COMPLETE dataset - NO date filters, ALL canonical transaction IDs\")\n    \n    export_dir = '../exports'\n    os.makedirs(export_dir, exist_ok=True)\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    \n    # Complete flat dataset - NO date filters, ALL canonical IDs\n    complete_flat_query = '''\n        SELECT \n            CanonicalTxID,\n            TransactionID,\n            DeviceID,\n            StoreID,\n            StoreName,\n            brand,\n            product_name,\n            category,\n            Amount,\n            Basket_Item_Count,\n            payment_method,\n            audio_transcript,\n            Txn_TS,\n            daypart,\n            weekday_weekend,\n            transaction_date\n        FROM gold.v_transactions_flat\n        -- NO DATE FILTER - GET ALL ROWS\n        ORDER BY CanonicalTxID\n    '''\n    \n    # Complete crosstab - ALL data\n    complete_crosstab_query = '''\n        SELECT \n            [date],\n            store_id,\n            store_name,\n            municipality_name,\n            daypart,\n            brand,\n            txn_count,\n            total_amount,\n            avg_basket_amount,\n            substitution_events\n        FROM gold.v_transactions_crosstab\n        -- NO DATE FILTER - GET ALL ROWS\n        ORDER BY [date] DESC, store_id, brand\n    '''\n    \n    with pyodbc.connect(conn_str) as conn:\n        print(\"ðŸ“Š Extracting COMPLETE flat dataset (no filters)...\")\n        df_complete_flat = pd.read_sql(complete_flat_query, conn)\n        \n        print(\"ðŸ“Š Extracting COMPLETE crosstab dataset (no filters)...\")\n        df_complete_crosstab = pd.read_sql(complete_crosstab_query, conn)\n    \n    # Export complete datasets\n    complete_flat_file = f'{export_dir}/scout_flat_COMPLETE_{timestamp}.csv'\n    complete_crosstab_file = f'{export_dir}/scout_crosstab_COMPLETE_{timestamp}.csv'\n    \n    df_complete_flat.to_csv(complete_flat_file, index=False)\n    df_complete_crosstab.to_csv(complete_crosstab_file, index=False)\n    \n    # Summary report\n    complete_summary = {\n        'export_timestamp': datetime.now().isoformat(),\n        'dataset_type': 'COMPLETE - NO FILTERS',\n        'files_created': {\n            'complete_flat': complete_flat_file,\n            'complete_crosstab': complete_crosstab_file\n        },\n        'row_counts': {\n            'flat_total': len(df_complete_flat),\n            'flat_with_timestamps': len(df_complete_flat[df_complete_flat['Txn_TS'].notna()]),\n            'flat_without_timestamps': len(df_complete_flat[df_complete_flat['Txn_TS'].isna()]),\n            'crosstab_total': len(df_complete_crosstab)\n        },\n        'data_quality': {\n            'unique_canonical_ids': df_complete_flat['CanonicalTxID'].nunique(),\n            'date_range': {\n                'min_date': str(df_complete_flat['transaction_date'].min()) if not df_complete_flat['transaction_date'].isna().all() else 'N/A',\n                'max_date': str(df_complete_flat['transaction_date'].max()) if not df_complete_flat['transaction_date'].isna().all() else 'N/A'\n            },\n            'stores_covered': df_complete_flat['StoreID'].nunique(),\n            'brands_covered': df_complete_flat['brand'].nunique()\n        }\n    }\n    \n    # Save summary\n    summary_file = f'{export_dir}/COMPLETE_export_summary_{timestamp}.json'\n    with open(summary_file, 'w') as f:\n        json.dump(complete_summary, f, indent=2)\n    \n    print(f\"\\nâœ… COMPLETE FLAT DATASET: {complete_flat_file}\")\n    print(f\"   ðŸ“Š Total rows: {len(df_complete_flat):,}\")\n    print(f\"   âœ… With timestamps: {len(df_complete_flat[df_complete_flat['Txn_TS'].notna()]):,}\")\n    print(f\"   âš ï¸  Without timestamps: {len(df_complete_flat[df_complete_flat['Txn_TS'].isna()]):,}\")\n    print(f\"   ðŸ†” Unique canonical IDs: {df_complete_flat['CanonicalTxID'].nunique():,}\")\n    \n    print(f\"\\nâœ… COMPLETE CROSSTAB DATASET: {complete_crosstab_file}\")\n    print(f\"   ðŸ“Š Total rows: {len(df_complete_crosstab):,}\")\n    \n    print(f\"\\nðŸ“‹ Summary report: {summary_file}\")\n    \n    # Final answer to your question\n    total_canonical_ids = df_complete_flat['CanonicalTxID'].nunique()\n    total_rows = len(df_complete_flat)\n    \n    if total_rows >= 12000:\n        print(f\"\\nðŸŽ¯ YOU HAVE {total_rows:,} TOTAL ROWS ({total_canonical_ids:,} unique canonical IDs)\")\n    elif total_rows >= 6000:\n        print(f\"\\nðŸŽ¯ YOU HAVE {total_rows:,} TOTAL ROWS ({total_canonical_ids:,} unique canonical IDs)\")\n    else:\n        print(f\"\\nðŸŽ¯ YOU HAVE {total_rows:,} TOTAL ROWS ({total_canonical_ids:,} unique canonical IDs)\")\n    \n    return complete_summary\n\n# Export complete dataset with no filters\ncomplete_export_summary = export_complete_dataset_no_filters()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 16. Full Dataset Export (Complete 12K+ Rows)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def export_with_identical_ordering():\n    \"\"\"Export with identical ordering to ensure data parity between views\"\"\"\n    \n    print(\"ðŸ”„ Exporting with identical ordering for data parity validation...\")\n    \n    export_dir = '../exports'\n    os.makedirs(export_dir, exist_ok=True)\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    \n    # 1. Export from gold.v_transactions_flat (source of truth)\n    flat_query = '''\n        SELECT *\n        FROM gold.v_transactions_flat\n        ORDER BY CanonicalTxID, TransactionID\n    '''\n    \n    # 2. Export from gold.v_transactions_flat_v24 (compatibility view)\n    v24_query = '''\n        SELECT *\n        FROM gold.v_transactions_flat_v24\n        ORDER BY CanonicalTxID, TransactionID\n    '''\n    \n    # 3. Export crosstab with consistent ordering\n    crosstab_query = '''\n        SELECT *\n        FROM gold.v_transactions_crosstab\n        ORDER BY [date] DESC, store_id, brand\n    '''\n    \n    with pyodbc.connect(conn_str) as conn:\n        print(\"ðŸ“Š Extracting flat view (gold.v_transactions_flat)...\")\n        df_flat = pd.read_sql(flat_query, conn)\n        \n        print(\"ðŸ“Š Extracting v24 compatibility view...\")\n        df_v24 = pd.read_sql(v24_query, conn)\n        \n        print(\"ðŸ“Š Extracting crosstab view...\")\n        df_crosstab = pd.read_sql(crosstab_query, conn)\n    \n    # Export with identical naming convention\n    flat_file = f'{export_dir}/scout_flat_ordered_{timestamp}.csv'\n    v24_file = f'{export_dir}/scout_v24_ordered_{timestamp}.csv'\n    crosstab_file = f'{export_dir}/scout_crosstab_ordered_{timestamp}.csv'\n    \n    df_flat.to_csv(flat_file, index=False)\n    df_v24.to_csv(v24_file, index=False)\n    df_crosstab.to_csv(crosstab_file, index=False)\n    \n    # Data parity validation\n    validation_results = {\n        'export_timestamp': datetime.now().isoformat(),\n        'files_created': {\n            'flat': flat_file,\n            'v24': v24_file,\n            'crosstab': crosstab_file\n        },\n        'row_counts': {\n            'flat': len(df_flat),\n            'v24': len(df_v24),\n            'crosstab': len(df_crosstab)\n        },\n        'column_counts': {\n            'flat': len(df_flat.columns),\n            'v24': len(df_v24.columns),\n            'crosstab': len(df_crosstab.columns)\n        },\n        'data_parity_check': {\n            'flat_v24_row_match': len(df_flat) == len(df_v24),\n            'flat_columns': list(df_flat.columns),\n            'v24_columns': list(df_v24.columns)\n        }\n    }\n    \n    # Save validation report\n    validation_file = f'{export_dir}/data_parity_validation_{timestamp}.json'\n    with open(validation_file, 'w') as f:\n        json.dump(validation_results, f, indent=2)\n    \n    print(f\"âœ… Flat view exported: {flat_file} ({len(df_flat):,} rows)\")\n    print(f\"âœ… V24 view exported: {v24_file} ({len(df_v24):,} rows)\")\n    print(f\"âœ… Crosstab exported: {crosstab_file} ({len(df_crosstab):,} rows)\")\n    print(f\"ðŸ“‹ Validation report: {validation_file}\")\n    \n    # Data quality summary\n    print(f\"\\nðŸ” Data Parity Validation:\")\n    print(f\"  Flat vs V24 row count match: {validation_results['data_parity_check']['flat_v24_row_match']}\")\n    print(f\"  Flat columns: {len(df_flat.columns)}\")\n    print(f\"  V24 columns: {len(df_v24.columns)}\")\n    print(f\"  Crosstab records: {len(df_crosstab):,}\")\n    \n    if validation_results['data_parity_check']['flat_v24_row_match']:\n        print(\"âœ… Data parity validation PASSED\")\n    else:\n        print(\"âš ï¸  Data parity validation needs review\")\n        print(f\"   Flat rows: {len(df_flat)} vs V24 rows: {len(df_v24)}\")\n    \n    return validation_results\n\n# Run identical ordering export for data parity\nparity_results = export_with_identical_ordering()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def export_enhanced_excel_crosstab():\n    \"\"\"Export enhanced crosstab with business research framework metrics\"\"\"\n    \n    # Extract comprehensive crosstab with all business metrics\n    enhanced_crosstab_query = '''\n        WITH DaypartBrandMetrics AS (\n            SELECT \n                CAST(flat.transaction_date AS date) as [date],\n                flat.store_id,\n                flat.StoreName,\n                'Metro Manila' as municipality_name,  -- Simplified for now\n                flat.daypart,\n                flat.brand,\n                flat.category,\n                flat.weekday_weekend,\n                COUNT(DISTINCT flat.CanonicalTxID) as txn_count,\n                SUM(flat.Amount) as total_amount,\n                AVG(flat.Amount) as avg_transaction_value,\n                SUM(flat.Basket_Item_Count) as total_items,\n                AVG(flat.Basket_Item_Count) as avg_basket_amount,\n                COUNT(DISTINCT CASE WHEN flat.payment_method IS NOT NULL THEN flat.payment_method END) as payment_methods_used,\n                \n                -- Business intelligence metrics\n                CASE \n                    WHEN flat.daypart = 'Morning' AND flat.category LIKE '%snack%' THEN 'Morning_Snack_Peak'\n                    WHEN flat.daypart = 'Evening' AND flat.category LIKE '%beverage%' THEN 'Evening_Beverage_Peak'\n                    ELSE 'Regular_Pattern'\n                END as peak_pattern,\n                \n                -- Substitution risk indicators (placeholder)\n                CASE \n                    WHEN flat.brand IN ('Oishi', 'Jack n Jill') THEN 1\n                    WHEN flat.brand IN ('Bear Brand', 'Alaska') THEN 1\n                    ELSE 0\n                END as substitution_events\n                \n            FROM gold.v_transactions_flat flat\n            WHERE flat.Txn_TS IS NOT NULL\n            AND flat.transaction_date >= DATEADD(day, -90, SYSUTCDATETIME())  -- Last 90 days\n            GROUP BY \n                CAST(flat.transaction_date AS date),\n                flat.store_id,\n                flat.StoreName,\n                flat.daypart,\n                flat.brand,\n                flat.category,\n                flat.weekday_weekend\n        )\n        SELECT \n            [date],\n            store_id,\n            StoreName as store_name,\n            municipality_name,\n            daypart,\n            brand,\n            category,\n            weekday_weekend,\n            txn_count,\n            total_amount,\n            avg_transaction_value,\n            total_items,\n            avg_basket_amount,\n            payment_methods_used,\n            peak_pattern,\n            substitution_events\n        FROM DaypartBrandMetrics\n        ORDER BY [date] DESC, total_amount DESC\n    '''\n    \n    with pyodbc.connect(conn_str) as conn:\n        df_enhanced_crosstab = pd.read_sql(enhanced_crosstab_query, conn)\n    \n    # Create export directory\n    export_dir = '../exports'\n    os.makedirs(export_dir, exist_ok=True)\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    \n    # Excel export with multiple sheets\n    excel_file = f'{export_dir}/scout_crosstab_enhanced_{timestamp}.xlsx'\n    \n    with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n        # Main crosstab data\n        df_enhanced_crosstab.to_excel(writer, sheet_name='Enhanced_Crosstab', index=False)\n        \n        # Business analytics sheets\n        \n        # 1. Daypart Analysis (Time of Day Framework)\n        daypart_analysis = df_enhanced_crosstab.groupby(['daypart', 'category']).agg({\n            'txn_count': 'sum',\n            'total_amount': 'sum',\n            'avg_transaction_value': 'mean'\n        }).round(2).reset_index()\n        daypart_analysis.to_excel(writer, sheet_name='Daypart_Analysis', index=False)\n        \n        # 2. Brand Performance by Daypart\n        brand_daypart = df_enhanced_crosstab.groupby(['brand', 'daypart']).agg({\n            'txn_count': 'sum',\n            'total_amount': 'sum',\n            'substitution_events': 'sum'\n        }).round(2).reset_index()\n        brand_daypart.to_excel(writer, sheet_name='Brand_Daypart', index=False)\n        \n        # 3. Weekend vs Weekday Patterns\n        weekend_analysis = df_enhanced_crosstab.groupby(['weekday_weekend', 'category']).agg({\n            'txn_count': 'sum',\n            'total_amount': 'sum',\n            'avg_basket_amount': 'mean'\n        }).round(2).reset_index()\n        weekend_analysis.to_excel(writer, sheet_name='Weekend_Patterns', index=False)\n        \n        # 4. Store Performance Rankings\n        store_performance = df_enhanced_crosstab.groupby(['store_id', 'store_name']).agg({\n            'txn_count': 'sum',\n            'total_amount': 'sum',\n            'avg_transaction_value': 'mean',\n            'substitution_events': 'sum'\n        }).round(2).reset_index().sort_values('total_amount', ascending=False)\n        store_performance.to_excel(writer, sheet_name='Store_Performance', index=False)\n        \n        # 5. Peak Pattern Summary\n        peak_summary = df_enhanced_crosstab.groupby('peak_pattern').agg({\n            'txn_count': 'sum',\n            'total_amount': 'sum',\n            'brand': 'nunique'\n        }).round(2).reset_index()\n        peak_summary.to_excel(writer, sheet_name='Peak_Patterns', index=False)\n    \n    print(f\"âœ… Enhanced Excel crosstab exported: {excel_file}\")\n    print(f\"ðŸ“Š Sheets created: Enhanced_Crosstab, Daypart_Analysis, Brand_Daypart, Weekend_Patterns, Store_Performance, Peak_Patterns\")\n    print(f\"ðŸ“ˆ Records: {len(df_enhanced_crosstab):,} rows\")\n    \n    return excel_file, df_enhanced_crosstab\n\ndef export_enriched_flat_csv():\n    \"\"\"Export enriched flat transactions with all business context\"\"\"\n    \n    # Extract comprehensive flat data with enrichments\n    enriched_flat_query = '''\n        SELECT \n            flat.CanonicalTxID,\n            flat.TransactionID,\n            flat.DeviceID,\n            flat.StoreID,\n            flat.StoreName,\n            flat.brand,\n            flat.product_name,\n            flat.category,\n            flat.Amount,\n            flat.Basket_Item_Count,\n            flat.payment_method,\n            flat.audio_transcript,\n            flat.Txn_TS,\n            flat.daypart,\n            flat.weekday_weekend,\n            flat.transaction_date,\n            \n            -- Business intelligence enrichments\n            CASE \n                WHEN flat.Amount > 500 THEN 'High_Value'\n                WHEN flat.Amount > 100 THEN 'Medium_Value'\n                ELSE 'Low_Value'\n            END as transaction_value_tier,\n            \n            CASE \n                WHEN flat.Basket_Item_Count > 10 THEN 'Large_Basket'\n                WHEN flat.Basket_Item_Count > 5 THEN 'Medium_Basket'\n                ELSE 'Small_Basket'\n            END as basket_size_tier,\n            \n            CASE \n                WHEN flat.daypart = 'Morning' AND flat.weekday_weekend = 'weekday' THEN 'Commuter_Rush'\n                WHEN flat.daypart = 'Evening' AND flat.weekday_weekend = 'weekend' THEN 'Social_Evening'\n                WHEN flat.daypart = 'Afternoon' AND flat.weekday_weekend = 'weekend' THEN 'Family_Shopping'\n                ELSE 'Regular_Shopping'\n            END as shopping_context,\n            \n            -- Customer behavior indicators\n            CASE \n                WHEN flat.category LIKE '%snack%' AND flat.daypart = 'Morning' THEN 'Impulse_Snacking'\n                WHEN flat.category LIKE '%beverage%' AND flat.daypart = 'Evening' THEN 'Social_Drinking'\n                WHEN flat.category LIKE '%essential%' THEN 'Necessity_Purchase'\n                ELSE 'Regular_Purchase'\n            END as purchase_motivation,\n            \n            -- Geographic context (simplified for demo)\n            CASE \n                WHEN flat.StoreID IN (102, 103) THEN 'Urban_Core'\n                WHEN flat.StoreID IN (109, 110) THEN 'Suburban'\n                ELSE 'Rural'\n            END as location_type,\n            \n            -- Time-based features for analysis\n            DATEPART(hour, flat.Txn_TS) as transaction_hour,\n            DATEPART(dayofweek, flat.transaction_date) as day_of_week,\n            DATEPART(month, flat.transaction_date) as transaction_month\n            \n        FROM gold.v_transactions_flat flat\n        WHERE flat.Txn_TS IS NOT NULL\n        AND flat.transaction_date >= DATEADD(day, -90, SYSUTCDATETIME())  -- Last 90 days\n        ORDER BY flat.transaction_date DESC, flat.Txn_TS DESC\n    '''\n    \n    with pyodbc.connect(conn_str) as conn:\n        df_enriched_flat = pd.read_sql(enriched_flat_query, conn)\n    \n    # Create export directory\n    export_dir = '../exports'\n    os.makedirs(export_dir, exist_ok=True)\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    \n    # CSV export\n    csv_file = f'{export_dir}/scout_flat_enriched_complete_{timestamp}.csv'\n    df_enriched_flat.to_csv(csv_file, index=False)\n    \n    # Create summary statistics\n    summary_stats = {\n        'total_transactions': len(df_enriched_flat),\n        'date_range': {\n            'start': df_enriched_flat['transaction_date'].min(),\n            'end': df_enriched_flat['transaction_date'].max()\n        },\n        'value_distribution': df_enriched_flat['transaction_value_tier'].value_counts().to_dict(),\n        'basket_distribution': df_enriched_flat['basket_size_tier'].value_counts().to_dict(),\n        'shopping_context': df_enriched_flat['shopping_context'].value_counts().to_dict(),\n        'location_distribution': df_enriched_flat['location_type'].value_counts().to_dict()\n    }\n    \n    # Save summary as JSON\n    summary_file = f'{export_dir}/scout_flat_summary_{timestamp}.json'\n    with open(summary_file, 'w') as f:\n        json.dump(summary_stats, f, indent=2, default=str)\n    \n    print(f\"âœ… Enriched flat CSV exported: {csv_file}\")\n    print(f\"ðŸ“Š Summary statistics: {summary_file}\")\n    print(f\"ðŸ“ˆ Records: {len(df_enriched_flat):,} rows\")\n    print(f\"ðŸ“… Date range: {summary_stats['date_range']['start']} to {summary_stats['date_range']['end']}\")\n    print(f\"ðŸ’° Value tiers: {summary_stats['value_distribution']}\")\n    \n    return csv_file, df_enriched_flat, summary_stats\n\n# Export enhanced files\nprint(\"ðŸš€ Starting enhanced export process...\")\nexcel_file, df_crosstab = export_enhanced_excel_crosstab()\ncsv_file, df_flat, summary = export_enriched_flat_csv()\n\nprint(f\"\\nðŸ“‹ Enhanced Export Summary:\")\nprint(f\"âœ… Excel crosstab with business analytics: {excel_file}\")\nprint(f\"âœ… Enriched flat CSV with context: {csv_file}\")\nprint(f\"ðŸ“Š Total data points: {len(df_crosstab) + len(df_flat):,}\")\nprint(f\"ðŸŽ¯ Ready for business research framework analysis\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def run_ai_enhanced_etl_pipeline(export_full=False):\n    \"\"\"Complete AI-enhanced ETL pipeline with CRISP-DM methodology\"\"\"\n    \n    pipeline_start = datetime.now()\n    print(f\"AI-Enhanced ETL Pipeline started at: {pipeline_start}\")\n    print(\"=\" * 60)\n    \n    try:\n        # CRISP-DM Phase 1: Business Understanding (already defined)\n        print(\"\\\\nCRISP-DM PHASE 1: Business Understanding âœ…\")\n        print(\"- Customer segmentation requirements defined\")\n        print(\"- CLV prediction objectives established\")\n        print(\"- Anomaly detection goals outlined\")\n        \n        # CRISP-DM Phase 2: Data Understanding\n        print(\"\\\\nCRISP-DM PHASE 2: Data Understanding\")\n        health = run_health_check()\n        customer_features = extract_customer_features()\n        print(f\"âœ… Extracted {len(customer_features)} customer profiles\")\n        \n        # CRISP-DM Phase 3: Data Preparation\n        print(\"\\\\nCRISP-DM PHASE 3: Data Preparation\")\n        X_raw, X_scaled, scaler, feature_cols = prepare_ml_features(customer_features)\n        print(f\"âœ… Prepared {len(feature_cols)} ML features\")\n        \n        # CRISP-DM Phase 4: Modeling\n        print(\"\\\\nCRISP-DM PHASE 4: Modeling\")\n        \n        # Customer Segmentation\n        seg_model, clusters, cluster_stats = build_customer_segmentation_model(X_scaled.values)\n        print(f\"âœ… Built customer segmentation model with {len(cluster_stats)} segments\")\n        \n        # CLV Prediction\n        clv_model, clv_metrics, clv_importance = build_clv_prediction_model(customer_features, X_raw)\n        if clv_model:\n            print(f\"âœ… Built CLV prediction model (RÂ² = {clv_metrics['r2']:.3f})\")\n        \n        # Anomaly Detection\n        anomaly_model, anomaly_labels, anomaly_scores = build_anomaly_detection_model(X_scaled.values)\n        anomaly_count = (anomaly_labels == -1).sum()\n        print(f\"âœ… Built anomaly detection model ({anomaly_count} anomalies detected)\")\n        \n        # CRISP-DM Phase 5: Evaluation\n        print(\"\\\\nCRISP-DM PHASE 5: Evaluation\")\n        saved_models = save_models_and_create_scoring_functions()\n        print(\"âœ… Models evaluated and saved for production\")\n        \n        # CRISP-DM Phase 6: Deployment\n        print(\"\\\\nCRISP-DM PHASE 6: Deployment\")\n        \n        # Export enhanced customer data with ML predictions\n        enhanced_customers = customer_features.copy()\n        enhanced_customers['customer_segment'] = clusters\n        enhanced_customers['segment_name'] = enhanced_customers['customer_segment'].map(\n            dict(zip(cluster_stats['cluster'], cluster_stats['cluster_name']))\n        )\n        enhanced_customers['is_anomaly'] = (anomaly_labels == -1).astype(int)\n        enhanced_customers['anomaly_score'] = anomaly_scores\n        \n        if clv_model:\n            purchase_mask = enhanced_customers['total_purchases'] > 0\n            enhanced_customers.loc[purchase_mask, 'predicted_clv'] = clv_model.predict(X_raw[purchase_mask])\n            enhanced_customers['predicted_clv'] = enhanced_customers['predicted_clv'].fillna(0)\n        \n        # Export AI-enhanced datasets\n        export_dir = '../exports'\n        os.makedirs(export_dir, exist_ok=True)\n        \n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        \n        # Enhanced customer analytics export\n        customer_file = f'{export_dir}/scout_customer_analytics_ai_{timestamp}.csv'\n        enhanced_customers.to_csv(customer_file, index=False)\n        print(f\"âœ… Exported AI-enhanced customer analytics: {customer_file}\")\n        \n        # Business intelligence dashboard data\n        dashboard_data = {\n            'segment_summary': cluster_stats.to_dict('records'),\n            'anomaly_summary': {\n                'total_customers': len(enhanced_customers),\n                'anomalies_detected': enhanced_customers['is_anomaly'].sum(),\n                'anomaly_rate': enhanced_customers['is_anomaly'].mean()\n            },\n            'clv_summary': {\n                'avg_predicted_clv': enhanced_customers.get('predicted_clv', pd.Series([0])).mean(),\n                'high_value_customers': len(enhanced_customers[enhanced_customers.get('predicted_clv', 0) > 1000])\n            } if clv_model else None\n        }\n        \n        dashboard_file = f'{export_dir}/scout_dashboard_data_{timestamp}.json'\n        with open(dashboard_file, 'w') as f:\n            json.dump(dashboard_data, f, indent=2, default=str)\n        print(f\"âœ… Exported dashboard data: {dashboard_file}\")\n        \n        # Traditional ETL exports (if requested)\n        if export_full:\n            traditional_summary = run_full_export_pipeline()\n            print(\"âœ… Completed traditional ETL exports\")\n        \n        # Pipeline success summary\n        pipeline_end = datetime.now()\n        duration = (pipeline_end - pipeline_start).total_seconds()\n        \n        ai_pipeline_summary = {\n            'status': 'success',\n            'pipeline_type': 'AI-Enhanced ETL with CRISP-DM',\n            'duration_seconds': duration,\n            'crisp_dm_phases_completed': 6,\n            'models_built': {\n                'customer_segmentation': True,\n                'clv_prediction': clv_model is not None,\n                'anomaly_detection': True\n            },\n            'exports_created': {\n                'customer_analytics': customer_file,\n                'dashboard_data': dashboard_file,\n                'traditional_etl': export_full\n            },\n            'ml_metrics': {\n                'customers_analyzed': len(enhanced_customers),\n                'segments_created': len(cluster_stats),\n                'anomalies_detected': enhanced_customers['is_anomaly'].sum(),\n                'clv_r2_score': clv_metrics['r2'] if clv_model else None\n            }\n        }\n        \n        print(f\"\\\\nðŸŽ¯ AI-Enhanced ETL Pipeline completed successfully!\")\n        print(f\"â±ï¸  Duration: {duration:.2f} seconds\")\n        print(f\"ðŸ“Š CRISP-DM methodology: All 6 phases completed\")\n        print(f\"ðŸ¤– ML models deployed and ready for production\")\n        \n        return ai_pipeline_summary\n        \n    except Exception as e:\n        print(f\"\\\\nâŒ AI-Enhanced ETL Pipeline failed: {str(e)}\")\n        return {\n            'status': 'failed',\n            'error': str(e),\n            'pipeline_type': 'AI-Enhanced ETL with CRISP-DM'\n        }\n\n# Run AI-Enhanced ETL Pipeline\nai_result = run_ai_enhanced_etl_pipeline(export_full=False)\nprint(f\"\\\\nðŸ“‹ Final Pipeline Result:\")\nprint(json.dumps(ai_result, indent=2, default=str))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 14. Enhanced ETL Pipeline with AI/ML Integration",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def save_models_and_create_scoring_functions():\n    \"\"\"Save trained models and create production scoring functions\"\"\"\n    \n    # Create models directory\n    models_dir = '../models'\n    os.makedirs(models_dir, exist_ok=True)\n    \n    # Save models\n    model_files = {}\n    \n    try:\n        # Save segmentation model\n        joblib.dump(segmentation_model, f'{models_dir}/customer_segmentation_model.pkl')\n        model_files['segmentation'] = f'{models_dir}/customer_segmentation_model.pkl'\n        \n        # Save scaler\n        joblib.dump(scaler, f'{models_dir}/feature_scaler.pkl')\n        model_files['scaler'] = f'{models_dir}/feature_scaler.pkl'\n        \n        # Save CLV model if available\n        if clv_model is not None:\n            joblib.dump(clv_model, f'{models_dir}/clv_prediction_model.pkl')\n            model_files['clv'] = f'{models_dir}/clv_prediction_model.pkl'\n        \n        # Save anomaly detection model\n        joblib.dump(anomaly_model, f'{models_dir}/anomaly_detection_model.pkl')\n        model_files['anomaly'] = f'{models_dir}/anomaly_detection_model.pkl'\n        \n        print(f\"Models saved successfully:\")\n        for model_type, file_path in model_files.items():\n            print(f\"  {model_type}: {file_path}\")\n        \n    except Exception as e:\n        print(f\"Error saving models: {str(e)}\")\n        return None\n    \n    return model_files\n\ndef create_production_scoring_function():\n    \"\"\"Create production-ready scoring function for new customers\"\"\"\n    \n    scoring_function = '''\ndef score_new_customer(customer_features, models_dir='../models'):\n    \\\"\\\"\\\"\n    Score new customer with segmentation, CLV prediction, and anomaly detection\n    \n    Args:\n        customer_features (dict): Customer feature dictionary\n        models_dir (str): Path to saved models\n    \n    Returns:\n        dict: Scoring results\n    \\\"\\\"\\\"\n    import joblib\n    import pandas as pd\n    import numpy as np\n    \n    # Load models\n    segmentation_model = joblib.load(f'{models_dir}/customer_segmentation_model.pkl')\n    scaler = joblib.load(f'{models_dir}/feature_scaler.pkl')\n    anomaly_model = joblib.load(f'{models_dir}/anomaly_detection_model.pkl')\n    \n    try:\n        clv_model = joblib.load(f'{models_dir}/clv_prediction_model.pkl')\n    except:\n        clv_model = None\n    \n    # Prepare features\n    feature_cols = [\n        'total_interactions', 'stores_visited', 'customer_lifetime_days',\n        'total_purchases', 'total_spent', 'avg_transaction_value',\n        'total_items_purchased', 'brands_purchased', 'dayparts_active',\n        'avg_interactions_per_day', 'purchase_conversion_rate', \n        'avg_items_per_transaction', 'weekend_preference',\n        'recency_score', 'frequency_score', 'monetary_score',\n        'engagement_intensity', 'cross_store_mobility', 'brand_loyalty'\n    ]\n    \n    # Convert to DataFrame and scale\n    X = pd.DataFrame([customer_features])[feature_cols]\n    X_scaled = scaler.transform(X)\n    \n    # Make predictions\n    segment = segmentation_model.predict(X_scaled)[0]\n    anomaly_score = anomaly_model.decision_function(X_scaled)[0]\n    is_anomaly = anomaly_model.predict(X_scaled)[0] == -1\n    \n    clv_prediction = None\n    if clv_model is not None and customer_features.get('total_purchases', 0) > 0:\n        clv_prediction = clv_model.predict(X)[0]\n    \n    # Cluster name mapping\n    cluster_names = {0: 'VIP_High_Value', 1: 'Premium_Loyal', 2: 'Regular_Active', \n                    3: 'Casual_Shoppers', 4: 'New_Customers'}\n    \n    return {\n        'customer_segment': segment,\n        'segment_name': cluster_names.get(segment, 'Unknown'),\n        'predicted_clv': clv_prediction,\n        'anomaly_score': anomaly_score,\n        'is_anomaly': is_anomaly,\n        'risk_level': 'HIGH' if is_anomaly else 'LOW'\n    }\n    '''\n    \n    # Save scoring function to file\n    with open('../models/scoring_function.py', 'w') as f:\n        f.write(scoring_function)\n    \n    print(\"Production scoring function created: ../models/scoring_function.py\")\n    \n    return scoring_function\n\n# Save models and create scoring functions\nsaved_models = save_models_and_create_scoring_functions()\nscoring_func = create_production_scoring_function()\n\n# Model evaluation summary\nevaluation_summary = {\n    'segmentation_model': {\n        'algorithm': 'K-Means',\n        'clusters': len(cluster_analysis),\n        'silhouette_score': max([silhouette_score(X_scaled.values, customer_clusters)])\n    },\n    'clv_model': clv_metrics if clv_model is not None else None,\n    'anomaly_model': {\n        'algorithm': 'Isolation Forest',\n        'contamination_rate': 0.1,\n        'anomalies_detected': df_customers['is_anomaly'].sum()\n    }\n}\n\nprint(f\"\\\\nModel Evaluation Summary:\")\nprint(json.dumps(evaluation_summary, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 13. CRISP-DM Phase 5: Evaluation & Production Deployment",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def build_anomaly_detection_model(X_scaled):\n    \"\"\"Build Isolation Forest model for anomaly detection\"\"\"\n    \n    # Train Isolation Forest model\n    isolation_forest = IsolationForest(\n        contamination=0.1,  # Expect 10% anomalies\n        random_state=42,\n        n_estimators=100\n    )\n    \n    # Fit and predict anomalies\n    anomaly_labels = isolation_forest.fit_predict(X_scaled)\n    anomaly_scores = isolation_forest.decision_function(X_scaled)\n    \n    # Convert labels: -1 (anomaly) to 1, 1 (normal) to 0\n    is_anomaly = (anomaly_labels == -1).astype(int)\n    \n    print(f\"Anomaly Detection Results:\")\n    print(f\"Total customers analyzed: {len(X_scaled)}\")\n    print(f\"Anomalies detected: {is_anomaly.sum()} ({is_anomaly.mean()*100:.1f}%)\")\n    print(f\"Normal customers: {(1-is_anomaly).sum()} ({(1-is_anomaly.mean())*100:.1f}%)\")\n    \n    # Analyze anomaly characteristics\n    df_customers['is_anomaly'] = is_anomaly\n    df_customers['anomaly_score'] = anomaly_scores\n    \n    # Compare anomaly vs normal customer characteristics\n    comparison = df_customers.groupby('is_anomaly').agg({\n        'total_interactions': ['mean', 'median'],\n        'total_spent': ['mean', 'median'],\n        'stores_visited': ['mean', 'median'],\n        'customer_lifetime_days': ['mean', 'median'],\n        'purchase_conversion_rate': ['mean', 'median']\n    }).round(2)\n    \n    print(f\"\\\\nAnomaly vs Normal Customer Comparison:\")\n    print(comparison)\n    \n    # Identify top anomalies\n    top_anomalies = df_customers[df_customers['is_anomaly'] == 1].nsmallest(5, 'anomaly_score')\n    print(f\"\\\\nTop 5 Anomalous Customers:\")\n    print(top_anomalies[['FacialID', 'total_interactions', 'total_spent', 'stores_visited', 'anomaly_score']].to_string(index=False))\n    \n    return isolation_forest, anomaly_labels, anomaly_scores\n\n# Build anomaly detection model\nanomaly_model, anomaly_labels, anomaly_scores = build_anomaly_detection_model(X_scaled.values)\n\n# Summary of anomaly detection insights\nanomaly_summary = {\n    'total_customers': len(df_customers),\n    'anomalies_detected': df_customers['is_anomaly'].sum(),\n    'anomaly_rate': df_customers['is_anomaly'].mean(),\n    'avg_anomaly_score': df_customers[df_customers['is_anomaly'] == 1]['anomaly_score'].mean()\n}\n\nprint(f\"\\\\nAnomaly Detection Summary: {anomaly_summary}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 12. CRISP-DM Phase 4: Modeling - Anomaly Detection",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def build_clv_prediction_model(df_customers, X_raw):\n    \"\"\"Build Random Forest model for Customer Lifetime Value prediction\"\"\"\n    \n    # Prepare target variable (CLV)\n    # Using total_spent as proxy for historical CLV, predict future CLV\n    y = df_customers['total_spent'].values\n    \n    # Filter customers with sufficient purchase history\n    purchase_mask = df_customers['total_purchases'] > 0\n    X_clv = X_raw[purchase_mask].copy()\n    y_clv = y[purchase_mask]\n    \n    if len(X_clv) < 50:\n        print(\"Insufficient customers with purchase history for CLV modeling\")\n        return None, None, None\n    \n    # Split data for training and testing\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_clv, y_clv, test_size=0.2, random_state=42\n    )\n    \n    # Train Random Forest model\n    rf_model = RandomForestRegressor(\n        n_estimators=100,\n        max_depth=10,\n        min_samples_split=5,\n        random_state=42\n    )\n    \n    rf_model.fit(X_train, y_train)\n    \n    # Make predictions\n    y_pred = rf_model.predict(X_test)\n    \n    # Calculate metrics\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    \n    print(f\"CLV Prediction Model Performance:\")\n    print(f\"MSE: {mse:.2f}\")\n    print(f\"RÂ² Score: {r2:.3f}\")\n    print(f\"Training samples: {len(X_train)}\")\n    print(f\"Test samples: {len(X_test)}\")\n    \n    # Feature importance\n    feature_importance = pd.DataFrame({\n        'feature': X_clv.columns,\n        'importance': rf_model.feature_importances_\n    }).sort_values('importance', ascending=False)\n    \n    print(f\"\\\\nTop 10 Most Important Features:\")\n    print(feature_importance.head(10).to_string(index=False))\n    \n    # Generate CLV predictions for all customers\n    clv_predictions = rf_model.predict(X_raw[purchase_mask])\n    df_customers.loc[purchase_mask, 'predicted_clv'] = clv_predictions\n    df_customers['predicted_clv'] = df_customers['predicted_clv'].fillna(0)\n    \n    return rf_model, {'mse': mse, 'r2': r2}, feature_importance\n\n# Build CLV prediction model\nclv_model, clv_metrics, clv_feature_importance = build_clv_prediction_model(df_customers, X_raw)\n\nif clv_model is not None:\n    print(f\"\\\\nCLV Predictions Summary:\")\n    print(f\"Mean predicted CLV: â‚±{df_customers['predicted_clv'].mean():.2f}\")\n    print(f\"Max predicted CLV: â‚±{df_customers['predicted_clv'].max():.2f}\")\n    print(f\"Customers with CLV > â‚±1000: {(df_customers['predicted_clv'] > 1000).sum()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 11. CRISP-DM Phase 4: Modeling - CLV Prediction",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def build_customer_segmentation_model(X_scaled, n_clusters=5):\n    \"\"\"Build K-Means customer segmentation model\"\"\"\n    \n    # Determine optimal number of clusters using elbow method\n    inertias = []\n    silhouette_scores = []\n    k_range = range(2, 8)\n    \n    for k in k_range:\n        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n        kmeans.fit(X_scaled)\n        inertias.append(kmeans.inertia_)\n        silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\\n    \n    # Select best k based on silhouette score\n    best_k = k_range[np.argmax(silhouette_scores)]\n    print(f\"Optimal number of clusters: {best_k} (silhouette score: {max(silhouette_scores):.3f})\")\n    \n    # Train final model\n    kmeans_model = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n    cluster_labels = kmeans_model.fit_predict(X_scaled)\n    \n    # Calculate cluster characteristics\n    cluster_stats = pd.DataFrame()\n    for i in range(best_k):\n        cluster_mask = cluster_labels == i\n        cluster_data = X_raw[cluster_mask]\n        \n        stats = {\n            'cluster': i,\n            'count': cluster_mask.sum(),\n            'avg_interactions': cluster_data['total_interactions'].mean(),\n            'avg_spent': cluster_data['total_spent'].mean(),\n            'avg_stores': cluster_data['stores_visited'].mean(),\n            'avg_lifetime_days': cluster_data['customer_lifetime_days'].mean(),\n            'conversion_rate': cluster_data['purchase_conversion_rate'].mean()\n        }\n        cluster_stats = pd.concat([cluster_stats, pd.DataFrame([stats])], ignore_index=True)\n    \n    # Assign cluster names based on characteristics\n    cluster_stats = cluster_stats.sort_values('avg_spent', ascending=False).reset_index(drop=True)\n    cluster_names = ['VIP_High_Value', 'Premium_Loyal', 'Regular_Active', 'Casual_Shoppers', 'New_Customers']\n    cluster_stats['cluster_name'] = cluster_names[:len(cluster_stats)]\n    \n    print(\"\\\\nCluster Analysis:\")\n    print(cluster_stats.to_string(index=False))\n    \n    return kmeans_model, cluster_labels, cluster_stats\n\n# Build segmentation model\nsegmentation_model, customer_clusters, cluster_analysis = build_customer_segmentation_model(X_scaled.values)\n\n# Add cluster labels to customer data\ndf_customers['customer_segment'] = customer_clusters\ndf_customers['segment_name'] = df_customers['customer_segment'].map(\n    dict(zip(cluster_analysis['cluster'], cluster_analysis['cluster_name']))\n)\n\nprint(f\"\\\\nCustomer segment distribution:\")\nprint(df_customers['segment_name'].value_counts())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. CRISP-DM Phase 4: Modeling - Customer Segmentation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def prepare_ml_features(df_customers):\n    \"\"\"Prepare ML-ready features with scaling and encoding\"\"\"\n    \n    # Create a copy for ML processing\n    df_ml = df_customers.copy()\n    \n    # Fill missing values\n    numeric_cols = df_ml.select_dtypes(include=[np.number]).columns\n    df_ml[numeric_cols] = df_ml[numeric_cols].fillna(0)\n    \n    # Feature engineering: RFM-like metrics\n    df_ml['recency_score'] = (datetime.now() - pd.to_datetime(df_ml['last_seen'])).dt.days\n    df_ml['frequency_score'] = df_ml['total_interactions']\n    df_ml['monetary_score'] = df_ml['total_spent']\n    \n    # Customer engagement metrics\n    df_ml['engagement_intensity'] = df_ml['total_interactions'] / np.maximum(df_ml['customer_lifetime_days'], 1)\n    df_ml['cross_store_mobility'] = df_ml['stores_visited'] / np.maximum(df_ml['total_interactions'], 1)\n    df_ml['brand_loyalty'] = df_ml['brands_purchased'] / np.maximum(df_ml['total_purchases'], 1)\n    \n    # Select features for ML models\n    feature_cols = [\n        'total_interactions', 'stores_visited', 'customer_lifetime_days',\n        'total_purchases', 'total_spent', 'avg_transaction_value',\n        'total_items_purchased', 'brands_purchased', 'dayparts_active',\n        'avg_interactions_per_day', 'purchase_conversion_rate', \n        'avg_items_per_transaction', 'weekend_preference',\n        'recency_score', 'frequency_score', 'monetary_score',\n        'engagement_intensity', 'cross_store_mobility', 'brand_loyalty'\n    ]\n    \n    X = df_ml[feature_cols].copy()\n    \n    # Handle infinite values\n    X = X.replace([np.inf, -np.inf], 0)\n    \n    # Scale features for ML algorithms\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    X_scaled_df = pd.DataFrame(X_scaled, columns=feature_cols, index=X.index)\n    \n    print(f\"Prepared ML features: {X_scaled_df.shape}\")\n    print(f\"Feature summary:\\n{X.describe()}\")\n    \n    return X, X_scaled_df, scaler, feature_cols\n\n# Prepare ML features\nX_raw, X_scaled, scaler, feature_columns = prepare_ml_features(df_customers)\n\n# Data quality validation\nprint(f\"\\nData Quality Check:\")\nprint(f\"Missing values: {X_raw.isnull().sum().sum()}\")\nprint(f\"Infinite values: {np.isinf(X_raw.values).sum()}\")\nprint(f\"Feature correlation matrix shape: {X_raw.corr().shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. CRISP-DM Phase 2: Data Understanding & Phase 3: Data Preparation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def extract_customer_features():\n    \"\"\"Extract customer-centric features for ML modeling\"\"\"\n    \n    # Extract comprehensive customer data with FacialID grouping\n    customer_query = '''\n        SELECT \n            si.FacialID,\n            COUNT(DISTINCT si.InteractionID) as total_interactions,\n            COUNT(DISTINCT si.StoreID) as stores_visited,\n            COUNT(DISTINCT CAST(si.TransactionDate AS date)) as active_days,\n            MIN(si.TransactionDate) as first_seen,\n            MAX(si.TransactionDate) as last_seen,\n            DATEDIFF(day, MIN(si.TransactionDate), MAX(si.TransactionDate)) as customer_lifetime_days,\n            \n            -- Purchase behavior (from matched transactions)\n            COUNT(DISTINCT flat.CanonicalTxID) as total_purchases,\n            ISNULL(SUM(flat.Amount), 0) as total_spent,\n            ISNULL(AVG(flat.Amount), 0) as avg_transaction_value,\n            ISNULL(MAX(flat.Amount), 0) as max_transaction_value,\n            ISNULL(SUM(flat.Basket_Item_Count), 0) as total_items_purchased,\n            \n            -- Behavioral patterns\n            COUNT(DISTINCT flat.brand) as brands_purchased,\n            COUNT(DISTINCT flat.daypart) as dayparts_active,\n            COUNT(CASE WHEN flat.weekday_weekend = 'weekend' THEN 1 END) as weekend_purchases,\n            COUNT(CASE WHEN flat.weekday_weekend = 'weekday' THEN 1 END) as weekday_purchases\n            \n        FROM dbo.SalesInteractions si\n        LEFT JOIN gold.v_transactions_flat flat ON si.InteractionID = flat.CanonicalTxID\n        WHERE si.FacialID IS NOT NULL\n        GROUP BY si.FacialID\n        HAVING COUNT(DISTINCT si.InteractionID) >= 5  -- Minimum activity threshold\n        ORDER BY total_interactions DESC\n    '''\n    \n    with pyodbc.connect(conn_str) as conn:\n        df_customers = pd.read_sql(customer_query, conn)\n    \n    # Calculate derived features\n    df_customers['avg_interactions_per_day'] = df_customers['total_interactions'] / np.maximum(df_customers['customer_lifetime_days'], 1)\n    df_customers['purchase_conversion_rate'] = df_customers['total_purchases'] / df_customers['total_interactions']\n    df_customers['avg_items_per_transaction'] = df_customers['total_items_purchased'] / np.maximum(df_customers['total_purchases'], 1)\n    df_customers['weekend_preference'] = df_customers['weekend_purchases'] / (df_customers['weekend_purchases'] + df_customers['weekday_purchases'])\n    df_customers['clv_estimate'] = df_customers['total_spent'] * (df_customers['customer_lifetime_days'] / 30) * 0.1  # Simple CLV proxy\n    \n    print(f\"Extracted features for {len(df_customers):,} customers\")\n    print(f\"Feature columns: {list(df_customers.columns)}\")\n    \n    return df_customers\n\n# Extract customer features\ndf_customers = extract_customer_features()\nprint(f\"\\nCustomer feature dataset shape: {df_customers.shape}\")\nprint(f\"Top customers by interactions:\\n{df_customers[['FacialID', 'total_interactions', 'stores_visited', 'total_spent']].head()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Scout v7 AI-Enhanced ETL Pipeline\n\nCRISP-DM methodology ETL pipeline for Scout transaction data with AI/ML components, JSON safety, and canonical joins.\n\n## Features:\n- **CRISP-DM Methodology**: 6-phase data science workflow\n- **AI/ML Components**: Customer segmentation, CLV prediction, anomaly detection\n- **JSON Safety**: Malformed payload handling (ISJSON guards)\n- **Canonical Joins**: Transaction ID joins with timestamp authority\n- **Export Pipeline**: Clean CSV formats with ML-ready features\n- **Data Quality**: Comprehensive validation and monitoring\n- **Real-time Scoring**: Production-ready ML inference"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport pandas as pd\nimport pyodbc\nimport json\nfrom datetime import datetime, timedelta\nimport warnings\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestRegressor, IsolationForest\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score, silhouette_score\nimport joblib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\n\n# Database connection parameters\nSERVER = 'sqltbwaprojectscoutserver.database.windows.net'\nDATABASE = 'SQL-TBWA-ProjectScout-Reporting-Prod'\nUSERNAME = 'sqladmin'\nPASSWORD = 'Azure_pw26'\n\n# Connection string\nconn_str = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={SERVER};DATABASE={DATABASE};UID={USERNAME};PWD={PASSWORD}'\n\nprint(f\"Connecting to: {SERVER}/{DATABASE}\")\nprint(f\"Timestamp: {datetime.now()}\")\nprint(\"AI/ML Components: Customer Segmentation, CLV Prediction, Anomaly Detection\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_health_check():\n",
    "    \"\"\"Run comprehensive data quality checks\"\"\"\n",
    "    \n",
    "    health_queries = {\n",
    "        'json_health': '''\n",
    "            SELECT\n",
    "              'JSON_HEALTH' as check_type,\n",
    "              SUM(CASE WHEN ISJSON(payload_json)=1 THEN 1 ELSE 0 END) AS good_json,\n",
    "              SUM(CASE WHEN ISJSON(payload_json)=0 THEN 1 ELSE 0 END) AS bad_json,\n",
    "              COUNT(*) as total_payloads\n",
    "            FROM dbo.PayloadTransactions\n",
    "        ''',\n",
    "        \n",
    "        'timestamp_coverage': '''\n",
    "            SELECT\n",
    "              'TIMESTAMP_COVERAGE' as check_type,\n",
    "              COUNT(*) AS flat_rows,\n",
    "              SUM(CASE WHEN txn_ts IS NOT NULL THEN 1 ELSE 0 END) AS with_ts,\n",
    "              CAST(100.0 * SUM(CASE WHEN txn_ts IS NOT NULL THEN 1 ELSE 0 END) / COUNT(*) AS decimal(5,2)) AS coverage_pct\n",
    "            FROM dbo.v_transactions_flat_production\n",
    "        ''',\n",
    "        \n",
    "        'view_row_counts': '''\n",
    "            SELECT 'gold.v_transactions_flat' as view_name, COUNT(*) as row_count FROM gold.v_transactions_flat\n",
    "            UNION ALL\n",
    "            SELECT 'gold.v_transactions_crosstab', COUNT(*) FROM gold.v_transactions_crosstab\n",
    "            UNION ALL  \n",
    "            SELECT 'gold.v_transactions_flat_v24', COUNT(*) FROM gold.v_transactions_flat_v24\n",
    "        '''\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    with pyodbc.connect(conn_str) as conn:\n",
    "        for check_name, query in health_queries.items():\n",
    "            df = pd.read_sql(query, conn)\n",
    "            results[check_name] = df\n",
    "            print(f\"\\n{check_name.upper()}:\")\n",
    "            print(df.to_string(index=False))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run health check\n",
    "health_results = run_health_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_flat_transactions(days_back=30, limit=None):\n",
    "    \"\"\"Extract flat transaction data with optional filtering\"\"\"\n",
    "    \n",
    "    base_query = '''\n",
    "        SELECT \n",
    "            CanonicalTxID,\n",
    "            TransactionID,\n",
    "            DeviceID,\n",
    "            StoreID,\n",
    "            StoreName,\n",
    "            brand,\n",
    "            product_name,\n",
    "            category,\n",
    "            Amount,\n",
    "            Basket_Item_Count,\n",
    "            payment_method,\n",
    "            audio_transcript,\n",
    "            Txn_TS,\n",
    "            daypart,\n",
    "            weekday_weekend,\n",
    "            transaction_date\n",
    "        FROM gold.v_transactions_flat\n",
    "        WHERE Txn_TS IS NOT NULL\n",
    "    '''\n",
    "    \n",
    "    if days_back:\n",
    "        base_query += f\" AND transaction_date >= CONVERT(date, DATEADD(day,-{days_back},SYSUTCDATETIME()))\"\n",
    "    \n",
    "    base_query += \" ORDER BY transaction_date DESC, Txn_TS DESC\"\n",
    "    \n",
    "    if limit:\n",
    "        base_query = f\"SELECT TOP {limit} * FROM ({base_query}) t\"\n",
    "    \n",
    "    with pyodbc.connect(conn_str) as conn:\n",
    "        df = pd.read_sql(base_query, conn)\n",
    "    \n",
    "    print(f\"Extracted {len(df):,} flat transactions\")\n",
    "    return df\n",
    "\n",
    "def extract_crosstab_analytics(days_back=30):\n",
    "    \"\"\"Extract crosstab analytics data\"\"\"\n",
    "    \n",
    "    query = '''\n",
    "        SELECT \n",
    "            [date],\n",
    "            store_id,\n",
    "            store_name,\n",
    "            municipality_name,\n",
    "            daypart,\n",
    "            brand,\n",
    "            txn_count,\n",
    "            total_amount,\n",
    "            avg_basket_amount,\n",
    "            substitution_events\n",
    "        FROM gold.v_transactions_crosstab\n",
    "    '''\n",
    "    \n",
    "    if days_back:\n",
    "        query += f\" WHERE [date] >= CONVERT(date, DATEADD(day,-{days_back},SYSUTCDATETIME()))\"\n",
    "    \n",
    "    query += \" ORDER BY [date] DESC, total_amount DESC\"\n",
    "    \n",
    "    with pyodbc.connect(conn_str) as conn:\n",
    "        df = pd.read_sql(query, conn)\n",
    "    \n",
    "    print(f\"Extracted {len(df):,} crosstab analytics rows\")\n",
    "    return df\n",
    "\n",
    "def extract_v24_compatibility():\n",
    "    \"\"\"Extract v24 compatibility format\"\"\"\n",
    "    \n",
    "    query = 'SELECT * FROM gold.v_transactions_flat_v24 WHERE Txn_TS IS NOT NULL ORDER BY Txn_TS DESC'\n",
    "    \n",
    "    with pyodbc.connect(conn_str) as conn:\n",
    "        df = pd.read_sql(query, conn)\n",
    "    \n",
    "    print(f\"Extracted {len(df):,} v24 compatibility rows\")\n",
    "    return df\n",
    "\n",
    "# Test extraction functions\n",
    "print(\"Testing extraction functions...\")\n",
    "sample_flat = extract_flat_transactions(days_back=7, limit=100)\n",
    "sample_crosstab = extract_crosstab_analytics(days_back=7)\n",
    "print(f\"\\nSample flat shape: {sample_flat.shape}\")\n",
    "print(f\"Sample crosstab shape: {sample_crosstab.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Analysis & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_transaction_patterns(df_flat, df_crosstab):\n",
    "    \"\"\"Analyze transaction patterns and generate insights\"\"\"\n",
    "    \n",
    "    insights = {}\n",
    "    \n",
    "    # Time-based patterns\n",
    "    insights['daypart_analysis'] = df_flat.groupby('daypart').agg({\n",
    "        'CanonicalTxID': 'count',\n",
    "        'Amount': ['sum', 'mean']\n",
    "    }).round(2)\n",
    "    \n",
    "    # Brand performance\n",
    "    insights['top_brands'] = df_flat.groupby('brand').agg({\n",
    "        'CanonicalTxID': 'count',\n",
    "        'Amount': 'sum'\n",
    "    }).sort_values('Amount', ascending=False).head(10)\n",
    "    \n",
    "    # Store performance\n",
    "    insights['store_performance'] = df_flat.groupby(['StoreID', 'StoreName']).agg({\n",
    "        'CanonicalTxID': 'count',\n",
    "        'Amount': ['sum', 'mean']\n",
    "    }).round(2)\n",
    "    \n",
    "    # Payment method distribution\n",
    "    insights['payment_methods'] = df_flat['payment_method'].value_counts()\n",
    "    \n",
    "    # Date range\n",
    "    insights['date_range'] = {\n",
    "        'min_date': df_flat['transaction_date'].min(),\n",
    "        'max_date': df_flat['transaction_date'].max(),\n",
    "        'total_days': (df_flat['transaction_date'].max() - df_flat['transaction_date'].min()).days\n",
    "    }\n",
    "    \n",
    "    return insights\n",
    "\n",
    "# Generate insights\n",
    "print(\"Generating transaction insights...\")\n",
    "insights = analyze_transaction_patterns(sample_flat, sample_crosstab)\n",
    "\n",
    "print(\"\\nDAYPART ANALYSIS:\")\n",
    "print(insights['daypart_analysis'])\n",
    "\n",
    "print(\"\\nTOP BRANDS:\")\n",
    "print(insights['top_brands'].head())\n",
    "\n",
    "print(\"\\nDATE RANGE:\")\n",
    "print(insights['date_range'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(df, filename, export_dir='../exports'):\n",
    "    \"\"\"Export dataframe to CSV with timestamp\"\"\"\n",
    "    \n",
    "    # Create export directory if it doesn't exist\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    \n",
    "    # Add timestamp to filename\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    base_name = filename.replace('.csv', '')\n",
    "    timestamped_filename = f\"{base_name}_{timestamp}.csv\"\n",
    "    \n",
    "    filepath = os.path.join(export_dir, timestamped_filename)\n",
    "    \n",
    "    # Export to CSV\n",
    "    df.to_csv(filepath, index=False)\n",
    "    \n",
    "    print(f\"Exported {len(df):,} rows to {filepath}\")\n",
    "    print(f\"File size: {os.path.getsize(filepath) / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def run_full_export_pipeline():\n",
    "    \"\"\"Run complete export pipeline\"\"\"\n",
    "    \n",
    "    print(\"Starting full export pipeline...\")\n",
    "    \n",
    "    # Extract all data\n",
    "    print(\"\\n1. Extracting flat transactions...\")\n",
    "    df_flat = extract_flat_transactions(days_back=None)  # All data\n",
    "    \n",
    "    print(\"\\n2. Extracting crosstab analytics...\")\n",
    "    df_crosstab = extract_crosstab_analytics(days_back=None)  # All data\n",
    "    \n",
    "    print(\"\\n3. Extracting v24 compatibility...\")\n",
    "    df_v24 = extract_v24_compatibility()\n",
    "    \n",
    "    # Export to CSV\n",
    "    print(\"\\n4. Exporting to CSV...\")\n",
    "    flat_file = export_to_csv(df_flat, 'scout_flat_enriched_complete.csv')\n",
    "    crosstab_file = export_to_csv(df_crosstab, 'scout_crosstab_enriched_complete.csv')\n",
    "    v24_file = export_to_csv(df_v24, 'scout_v24_compatibility_complete.csv')\n",
    "    \n",
    "    # Generate summary\n",
    "    summary = {\n",
    "        'export_timestamp': datetime.now().isoformat(),\n",
    "        'files_created': [flat_file, crosstab_file, v24_file],\n",
    "        'row_counts': {\n",
    "            'flat': len(df_flat),\n",
    "            'crosstab': len(df_crosstab),\n",
    "            'v24': len(df_v24)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\n5. Export Summary:\")\n",
    "    print(json.dumps(summary, indent=2))\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Run sample export\n",
    "print(\"Running sample export (last 7 days)...\")\n",
    "sample_flat_7d = extract_flat_transactions(days_back=7)\n",
    "sample_crosstab_7d = extract_crosstab_analytics(days_back=7)\n",
    "\n",
    "export_to_csv(sample_flat_7d, 'scout_flat_sample_7d.csv')\n",
    "export_to_csv(sample_crosstab_7d, 'scout_crosstab_sample_7d.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Automated ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_etl_pipeline(export_full=False):\n",
    "    \"\"\"Complete ETL pipeline with health checks\"\"\"\n",
    "    \n",
    "    pipeline_start = datetime.now()\n",
    "    print(f\"ETL Pipeline started at: {pipeline_start}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Health Check\n",
    "        print(\"\\nSTEP 1: Running health checks...\")\n",
    "        health = run_health_check()\n",
    "        \n",
    "        # Step 2: Data Extraction\n",
    "        print(\"\\nSTEP 2: Extracting data...\")\n",
    "        if export_full:\n",
    "            summary = run_full_export_pipeline()\n",
    "        else:\n",
    "            # Sample export for testing\n",
    "            df_flat = extract_flat_transactions(days_back=30, limit=1000)\n",
    "            df_crosstab = extract_crosstab_analytics(days_back=30)\n",
    "            \n",
    "            export_to_csv(df_flat, 'scout_flat_sample.csv')\n",
    "            export_to_csv(df_crosstab, 'scout_crosstab_sample.csv')\n",
    "            \n",
    "            summary = {\n",
    "                'export_timestamp': datetime.now().isoformat(),\n",
    "                'mode': 'sample',\n",
    "                'row_counts': {\n",
    "                    'flat': len(df_flat),\n",
    "                    'crosstab': len(df_crosstab)\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Step 3: Success Summary\n",
    "        pipeline_end = datetime.now()\n",
    "        duration = (pipeline_end - pipeline_start).total_seconds()\n",
    "        \n",
    "        print(\"\\nSTEP 3: Pipeline completed successfully!\")\n",
    "        print(f\"Duration: {duration:.2f} seconds\")\n",
    "        print(f\"End time: {pipeline_end}\")\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'duration_seconds': duration,\n",
    "            'summary': summary,\n",
    "            'health_checks': health\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: Pipeline failed with error: {str(e)}\")\n",
    "        return {\n",
    "            'status': 'failed',\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# Run ETL pipeline (sample mode)\n",
    "result = run_etl_pipeline(export_full=False)\n",
    "print(\"\\nPipeline Result:\")\n",
    "print(json.dumps(result, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Production Export (Uncomment to run full export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to run full production export\n",
    "# production_result = run_etl_pipeline(export_full=True)\n",
    "# print(json.dumps(production_result, indent=2, default=str))\n",
    "\n",
    "print(\"To run full production export, uncomment the lines above and execute this cell.\")\n",
    "print(\"This will export ALL transaction data to timestamped CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 15. Enhanced Excel & CSV Export Functions",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 8. CRISP-DM Phase 1: Business Understanding\n\n### Customer Analytics Requirements\n- **Customer Segmentation**: Identify customer personas and behavior patterns\n- **Customer Lifetime Value (CLV)**: Predict future value and optimize acquisition\n- **Anomaly Detection**: Identify unusual patterns and potential fraud\n- **Real-time Scoring**: Enable production ML inference for new transactions",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}